{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('images', exist_ok=True)\n",
    "\n",
    "class Opt(object):\n",
    "    def __init__(self):\n",
    "        self.n_epochs = 200\n",
    "        self.batch_size = 64\n",
    "        self.lr = 0.0002\n",
    "        self.b1 = 0.5\n",
    "        self.b2 = 0.999\n",
    "        self.n_cpu = 8\n",
    "        self.latent_dim = 100\n",
    "        self.img_size = 64\n",
    "        self.channels = 1\n",
    "        self.sample_interval = 100\n",
    "        \n",
    "opt = Opt()\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--n_epochs', type=int, default=200, help='number of epochs of training')\n",
    "# parser.add_argument('--batch_size', type=int, default=64, help='size of the batches')\n",
    "# parser.add_argument('--lr', type=float, default=0.0002, help='adam: learning rate')\n",
    "# parser.add_argument('--b1', type=float, default=0.5, help='adam: decay of first order momentum of gradient')\n",
    "# parser.add_argument('--b2', type=float, default=0.999, help='adam: decay of first order momentum of gradient')\n",
    "# parser.add_argument('--n_cpu', type=int, default=8, help='number of cpu threads to use during batch generation')\n",
    "# parser.add_argument('--latent_dim', type=int, default=100, help='dimensionality of the latent space')\n",
    "# parser.add_argument('--img_size', type=int, default=28, help='size of each image dimension')\n",
    "# parser.add_argument('--channels', type=int, default=1, help='number of image channels')\n",
    "# parser.add_argument('--sample_interval', type=int, default=400, help='interval betwen image samples')\n",
    "# opt = parser.parse_args()\n",
    "# print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (opt.channels, opt.img_size, opt.img_size)\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    # initializers\n",
    "    def __init__(self, d=128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.deconv1 = nn.ConvTranspose2d(100, d*8, 4, 1, 0)\n",
    "        self.deconv1_bn = nn.BatchNorm2d(d*8)\n",
    "        self.deconv2 = nn.ConvTranspose2d(d*8, d*4, 4, 2, 1)\n",
    "        self.deconv2_bn = nn.BatchNorm2d(d*4)\n",
    "        self.deconv3 = nn.ConvTranspose2d(d*4, d*2, 4, 2, 1)\n",
    "        self.deconv3_bn = nn.BatchNorm2d(d*2)\n",
    "        self.deconv4 = nn.ConvTranspose2d(d*2, d, 4, 2, 1)\n",
    "        self.deconv4_bn = nn.BatchNorm2d(d)\n",
    "        self.deconv5 = nn.ConvTranspose2d(d, 1, 4, 2, 1)\n",
    "\n",
    "    # forward method\n",
    "    def forward(self, input):\n",
    "        x = F.relu(self.deconv1(input))\n",
    "        x = F.relu(self.deconv1_bn(self.deconv1(input)))\n",
    "        x = F.relu(self.deconv2_bn(self.deconv2(x)))\n",
    "        x = F.relu(self.deconv3_bn(self.deconv3(x)))\n",
    "        x = F.relu(self.deconv4_bn(self.deconv4(x)))\n",
    "        x = F.tanh(self.deconv5(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Discriminator(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Discriminator, self).__init__()\n",
    "\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(int(np.prod(img_shape)), 512),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.Linear(512, 256),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.Linear(256, 1),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, img):\n",
    "#         img_flat = img.view(img.size(0), -1)\n",
    "#         validity = self.model(img_flat)\n",
    "\n",
    "#         return validity\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    # initializers\n",
    "    def __init__(self, d=128):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, d, 4, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(d, d*2, 4, 2, 1)\n",
    "        self.conv2_bn = nn.BatchNorm2d(d*2)\n",
    "        self.conv3 = nn.Conv2d(d*2, d*4, 4, 2, 1)\n",
    "        self.conv3_bn = nn.BatchNorm2d(d*4)\n",
    "        self.conv4 = nn.Conv2d(d*4, d*8, 4, 2, 1)\n",
    "        self.conv4_bn = nn.BatchNorm2d(d*8)\n",
    "        self.conv5 = nn.Conv2d(d*8, 1, 4, 1, 0)\n",
    "        \n",
    "    # forward method\n",
    "    def forward(self, input):\n",
    "        x = F.leaky_relu(self.conv1(input), 0.2)\n",
    "        x = F.leaky_relu(self.conv2_bn(self.conv2(x)), 0.2)\n",
    "        x = F.leaky_relu(self.conv3_bn(self.conv3(x)), 0.2)\n",
    "        x = F.leaky_relu(self.conv4_bn(self.conv4(x)), 0.2)\n",
    "        x = F.sigmoid(self.conv5(x))\n",
    "\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "adversarial_loss = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/takuma2460_0131/.pyenv/versions/anaconda3-5.3.0/lib/python3.7/site-packages/torchvision/transforms/transforms.py:187: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n"
     ]
    }
   ],
   "source": [
    "# Configure data loader\n",
    "os.makedirs('../../data/mnist', exist_ok=True)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../../data/mnist', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.Scale(64),\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                   ])),\n",
    "    batch_size=opt.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-149-4123c17a08a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.0/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.0/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.0/lib/python3.7/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# doing this so that it is consistent with all other datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# to return a PIL Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.0/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2480\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2482\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.0/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfrombuffer\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   2426\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m  \u001b[0;31m# may change to (mode, 0, 1) post-1.1.6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2427\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_MAPMODES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2428\u001b[0;31m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2429\u001b[0m             im = im._new(\n\u001b[1;32m   2430\u001b[0m                 \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.0/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mnew\u001b[0;34m(mode, size, color)\u001b[0m\n\u001b[1;32m   2329\u001b[0m         \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageColor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2331\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, (imgs, _) in enumerate(dataloader):\n",
    "    print(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/takuma2460_0131/.pyenv/versions/anaconda3-5.3.0/lib/python3.7/site-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/home/takuma2460_0131/.pyenv/versions/anaconda3-5.3.0/lib/python3.7/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/100] [Batch 0/938] [D loss: 0.523147] [G loss: 0.635964]\n",
      "[Epoch 0/100] [Batch 1/938] [D loss: 0.969898] [G loss: 3.880957]\n",
      "[Epoch 0/100] [Batch 2/938] [D loss: 0.923864] [G loss: 0.260798]\n",
      "[Epoch 0/100] [Batch 3/938] [D loss: 0.459459] [G loss: 3.229306]\n",
      "[Epoch 0/100] [Batch 4/938] [D loss: 0.364646] [G loss: 1.172357]\n",
      "[Epoch 0/100] [Batch 5/938] [D loss: 0.525784] [G loss: 0.954748]\n",
      "[Epoch 0/100] [Batch 6/938] [D loss: 0.583977] [G loss: 1.738892]\n",
      "[Epoch 0/100] [Batch 7/938] [D loss: 0.720323] [G loss: 0.432933]\n",
      "[Epoch 0/100] [Batch 8/938] [D loss: 0.679327] [G loss: 2.694703]\n",
      "[Epoch 0/100] [Batch 9/938] [D loss: 0.401460] [G loss: 1.022782]\n",
      "[Epoch 0/100] [Batch 10/938] [D loss: 0.260115] [G loss: 1.896657]\n",
      "[Epoch 0/100] [Batch 11/938] [D loss: 0.304410] [G loss: 1.547145]\n",
      "[Epoch 0/100] [Batch 12/938] [D loss: 0.410694] [G loss: 1.527929]\n",
      "[Epoch 0/100] [Batch 13/938] [D loss: 0.505490] [G loss: 0.719875]\n",
      "[Epoch 0/100] [Batch 14/938] [D loss: 0.687703] [G loss: 2.303448]\n",
      "[Epoch 0/100] [Batch 15/938] [D loss: 0.505087] [G loss: 0.650905]\n",
      "[Epoch 0/100] [Batch 16/938] [D loss: 0.439385] [G loss: 2.051181]\n",
      "[Epoch 0/100] [Batch 17/938] [D loss: 0.385072] [G loss: 1.133964]\n",
      "[Epoch 0/100] [Batch 18/938] [D loss: 0.416349] [G loss: 1.285815]\n",
      "[Epoch 0/100] [Batch 19/938] [D loss: 0.391863] [G loss: 1.233177]\n",
      "[Epoch 0/100] [Batch 20/938] [D loss: 0.452958] [G loss: 1.303555]\n",
      "[Epoch 0/100] [Batch 21/938] [D loss: 0.387644] [G loss: 1.255025]\n",
      "[Epoch 0/100] [Batch 22/938] [D loss: 0.431392] [G loss: 1.337043]\n",
      "[Epoch 0/100] [Batch 23/938] [D loss: 0.471450] [G loss: 1.202627]\n",
      "[Epoch 0/100] [Batch 24/938] [D loss: 0.385105] [G loss: 0.937476]\n",
      "[Epoch 0/100] [Batch 25/938] [D loss: 0.290822] [G loss: 2.601082]\n",
      "[Epoch 0/100] [Batch 26/938] [D loss: 0.245298] [G loss: 1.427755]\n",
      "[Epoch 0/100] [Batch 27/938] [D loss: 0.310486] [G loss: 1.650156]\n",
      "[Epoch 0/100] [Batch 28/938] [D loss: 0.374929] [G loss: 1.063185]\n",
      "[Epoch 0/100] [Batch 29/938] [D loss: 0.525638] [G loss: 1.828468]\n",
      "[Epoch 0/100] [Batch 30/938] [D loss: 0.585913] [G loss: 0.523526]\n",
      "[Epoch 0/100] [Batch 31/938] [D loss: 0.954846] [G loss: 4.008315]\n",
      "[Epoch 0/100] [Batch 32/938] [D loss: 0.610114] [G loss: 0.517537]\n",
      "[Epoch 0/100] [Batch 33/938] [D loss: 0.307363] [G loss: 1.868767]\n",
      "[Epoch 0/100] [Batch 34/938] [D loss: 0.418579] [G loss: 1.606679]\n",
      "[Epoch 0/100] [Batch 35/938] [D loss: 0.438751] [G loss: 0.906242]\n",
      "[Epoch 0/100] [Batch 36/938] [D loss: 0.339938] [G loss: 1.769812]\n",
      "[Epoch 0/100] [Batch 37/938] [D loss: 0.345350] [G loss: 1.443375]\n",
      "[Epoch 0/100] [Batch 38/938] [D loss: 0.390411] [G loss: 1.107568]\n",
      "[Epoch 0/100] [Batch 39/938] [D loss: 0.436248] [G loss: 1.887729]\n",
      "[Epoch 0/100] [Batch 40/938] [D loss: 0.461271] [G loss: 0.815806]\n",
      "[Epoch 0/100] [Batch 41/938] [D loss: 0.457569] [G loss: 2.580047]\n",
      "[Epoch 0/100] [Batch 42/938] [D loss: 0.474948] [G loss: 0.792184]\n",
      "[Epoch 0/100] [Batch 43/938] [D loss: 0.287998] [G loss: 2.091702]\n",
      "[Epoch 0/100] [Batch 44/938] [D loss: 0.380348] [G loss: 1.402127]\n",
      "[Epoch 0/100] [Batch 45/938] [D loss: 0.276332] [G loss: 1.392314]\n",
      "[Epoch 0/100] [Batch 46/938] [D loss: 0.378419] [G loss: 1.534068]\n",
      "[Epoch 0/100] [Batch 47/938] [D loss: 0.304925] [G loss: 1.387010]\n",
      "[Epoch 0/100] [Batch 48/938] [D loss: 0.394431] [G loss: 1.665208]\n",
      "[Epoch 0/100] [Batch 49/938] [D loss: 0.544397] [G loss: 0.669736]\n",
      "[Epoch 0/100] [Batch 50/938] [D loss: 0.659180] [G loss: 3.562274]\n",
      "[Epoch 0/100] [Batch 51/938] [D loss: 0.427988] [G loss: 0.896324]\n",
      "[Epoch 0/100] [Batch 52/938] [D loss: 0.286031] [G loss: 1.851478]\n",
      "[Epoch 0/100] [Batch 53/938] [D loss: 0.307442] [G loss: 1.645462]\n",
      "[Epoch 0/100] [Batch 54/938] [D loss: 0.339202] [G loss: 1.958445]\n",
      "[Epoch 0/100] [Batch 55/938] [D loss: 0.389590] [G loss: 0.886364]\n",
      "[Epoch 0/100] [Batch 56/938] [D loss: 0.313364] [G loss: 2.812464]\n",
      "[Epoch 0/100] [Batch 57/938] [D loss: 0.335508] [G loss: 1.228947]\n",
      "[Epoch 0/100] [Batch 58/938] [D loss: 0.304327] [G loss: 1.558293]\n",
      "[Epoch 0/100] [Batch 59/938] [D loss: 0.351309] [G loss: 1.646895]\n",
      "[Epoch 0/100] [Batch 60/938] [D loss: 0.415821] [G loss: 1.079362]\n",
      "[Epoch 0/100] [Batch 61/938] [D loss: 0.377202] [G loss: 1.925696]\n",
      "[Epoch 0/100] [Batch 62/938] [D loss: 0.340655] [G loss: 1.147137]\n",
      "[Epoch 0/100] [Batch 63/938] [D loss: 0.244004] [G loss: 2.718514]\n",
      "[Epoch 0/100] [Batch 64/938] [D loss: 0.285016] [G loss: 1.366320]\n",
      "[Epoch 0/100] [Batch 65/938] [D loss: 0.299164] [G loss: 2.108354]\n",
      "[Epoch 0/100] [Batch 66/938] [D loss: 0.392007] [G loss: 0.935813]\n",
      "[Epoch 0/100] [Batch 67/938] [D loss: 0.569821] [G loss: 3.230184]\n",
      "[Epoch 0/100] [Batch 68/938] [D loss: 0.407444] [G loss: 0.778650]\n",
      "[Epoch 0/100] [Batch 69/938] [D loss: 0.304260] [G loss: 2.185536]\n",
      "[Epoch 0/100] [Batch 70/938] [D loss: 0.248347] [G loss: 1.907853]\n",
      "[Epoch 0/100] [Batch 71/938] [D loss: 0.251488] [G loss: 1.381348]\n",
      "[Epoch 0/100] [Batch 72/938] [D loss: 0.208463] [G loss: 1.776964]\n",
      "[Epoch 0/100] [Batch 73/938] [D loss: 0.481784] [G loss: 2.285229]\n",
      "[Epoch 0/100] [Batch 74/938] [D loss: 0.750103] [G loss: 0.369243]\n",
      "[Epoch 0/100] [Batch 75/938] [D loss: 0.888257] [G loss: 4.261894]\n",
      "[Epoch 0/100] [Batch 76/938] [D loss: 0.487418] [G loss: 0.724962]\n",
      "[Epoch 0/100] [Batch 77/938] [D loss: 0.370356] [G loss: 1.778616]\n",
      "[Epoch 0/100] [Batch 78/938] [D loss: 0.396654] [G loss: 1.570374]\n",
      "[Epoch 0/100] [Batch 79/938] [D loss: 0.321860] [G loss: 1.285620]\n",
      "[Epoch 0/100] [Batch 80/938] [D loss: 0.360827] [G loss: 1.801353]\n",
      "[Epoch 0/100] [Batch 81/938] [D loss: 0.231860] [G loss: 1.605326]\n",
      "[Epoch 0/100] [Batch 82/938] [D loss: 0.236879] [G loss: 1.877414]\n",
      "[Epoch 0/100] [Batch 83/938] [D loss: 0.362449] [G loss: 1.115649]\n",
      "[Epoch 0/100] [Batch 84/938] [D loss: 0.637965] [G loss: 2.103992]\n",
      "[Epoch 0/100] [Batch 85/938] [D loss: 0.827890] [G loss: 0.288201]\n",
      "[Epoch 0/100] [Batch 86/938] [D loss: 0.847065] [G loss: 3.577030]\n",
      "[Epoch 0/100] [Batch 87/938] [D loss: 0.387464] [G loss: 0.897345]\n",
      "[Epoch 0/100] [Batch 88/938] [D loss: 0.292653] [G loss: 1.710084]\n",
      "[Epoch 0/100] [Batch 89/938] [D loss: 0.379690] [G loss: 1.813361]\n",
      "[Epoch 0/100] [Batch 90/938] [D loss: 0.400810] [G loss: 0.919380]\n",
      "[Epoch 0/100] [Batch 91/938] [D loss: 0.299843] [G loss: 2.100363]\n",
      "[Epoch 0/100] [Batch 92/938] [D loss: 0.363728] [G loss: 1.358648]\n",
      "[Epoch 0/100] [Batch 93/938] [D loss: 0.569162] [G loss: 0.734917]\n",
      "[Epoch 0/100] [Batch 94/938] [D loss: 0.484456] [G loss: 1.949846]\n",
      "[Epoch 0/100] [Batch 95/938] [D loss: 0.496487] [G loss: 0.764735]\n",
      "[Epoch 0/100] [Batch 96/938] [D loss: 0.420105] [G loss: 2.860595]\n",
      "[Epoch 0/100] [Batch 97/938] [D loss: 0.321941] [G loss: 1.298993]\n",
      "[Epoch 0/100] [Batch 98/938] [D loss: 0.257257] [G loss: 1.445783]\n",
      "[Epoch 0/100] [Batch 99/938] [D loss: 0.442655] [G loss: 2.414723]\n",
      "[Epoch 0/100] [Batch 100/938] [D loss: 0.609188] [G loss: 0.468482]\n",
      "[Epoch 0/100] [Batch 101/938] [D loss: 0.639722] [G loss: 3.317270]\n",
      "[Epoch 0/100] [Batch 102/938] [D loss: 0.504288] [G loss: 0.685294]\n",
      "[Epoch 0/100] [Batch 103/938] [D loss: 0.388458] [G loss: 1.605750]\n",
      "[Epoch 0/100] [Batch 104/938] [D loss: 0.366308] [G loss: 2.066778]\n",
      "[Epoch 0/100] [Batch 105/938] [D loss: 0.249477] [G loss: 1.339449]\n",
      "[Epoch 0/100] [Batch 106/938] [D loss: 0.296521] [G loss: 1.643451]\n",
      "[Epoch 0/100] [Batch 107/938] [D loss: 0.343061] [G loss: 1.636298]\n",
      "[Epoch 0/100] [Batch 108/938] [D loss: 0.334961] [G loss: 1.065389]\n",
      "[Epoch 0/100] [Batch 109/938] [D loss: 0.621580] [G loss: 3.452308]\n",
      "[Epoch 0/100] [Batch 110/938] [D loss: 0.501218] [G loss: 0.605567]\n",
      "[Epoch 0/100] [Batch 111/938] [D loss: 0.353316] [G loss: 1.368918]\n",
      "[Epoch 0/100] [Batch 112/938] [D loss: 0.834094] [G loss: 3.036576]\n",
      "[Epoch 0/100] [Batch 113/938] [D loss: 0.392143] [G loss: 0.817623]\n",
      "[Epoch 0/100] [Batch 114/938] [D loss: 0.307892] [G loss: 1.335598]\n",
      "[Epoch 0/100] [Batch 115/938] [D loss: 0.593829] [G loss: 2.755081]\n",
      "[Epoch 0/100] [Batch 116/938] [D loss: 0.348018] [G loss: 0.976662]\n",
      "[Epoch 0/100] [Batch 117/938] [D loss: 0.273859] [G loss: 2.450463]\n",
      "[Epoch 0/100] [Batch 118/938] [D loss: 0.237853] [G loss: 1.513596]\n",
      "[Epoch 0/100] [Batch 119/938] [D loss: 0.196286] [G loss: 2.173686]\n",
      "[Epoch 0/100] [Batch 120/938] [D loss: 0.184389] [G loss: 2.186956]\n",
      "[Epoch 0/100] [Batch 121/938] [D loss: 0.126084] [G loss: 2.493946]\n",
      "[Epoch 0/100] [Batch 122/938] [D loss: 0.194626] [G loss: 1.584636]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/100] [Batch 123/938] [D loss: 0.374726] [G loss: 4.110402]\n",
      "[Epoch 0/100] [Batch 124/938] [D loss: 0.445136] [G loss: 0.680633]\n",
      "[Epoch 0/100] [Batch 125/938] [D loss: 0.298794] [G loss: 3.921082]\n",
      "[Epoch 0/100] [Batch 126/938] [D loss: 0.060161] [G loss: 3.508184]\n",
      "[Epoch 0/100] [Batch 127/938] [D loss: 0.511605] [G loss: 0.635408]\n",
      "[Epoch 0/100] [Batch 128/938] [D loss: 0.590527] [G loss: 4.038654]\n",
      "[Epoch 0/100] [Batch 129/938] [D loss: 0.311121] [G loss: 1.219172]\n",
      "[Epoch 0/100] [Batch 130/938] [D loss: 0.407693] [G loss: 1.000586]\n",
      "[Epoch 0/100] [Batch 131/938] [D loss: 0.622804] [G loss: 3.468388]\n",
      "[Epoch 0/100] [Batch 132/938] [D loss: 0.697546] [G loss: 0.431675]\n",
      "[Epoch 0/100] [Batch 133/938] [D loss: 0.306493] [G loss: 2.901717]\n",
      "[Epoch 0/100] [Batch 134/938] [D loss: 0.364427] [G loss: 2.232435]\n",
      "[Epoch 0/100] [Batch 135/938] [D loss: 0.590149] [G loss: 0.526680]\n",
      "[Epoch 0/100] [Batch 136/938] [D loss: 0.427059] [G loss: 2.185306]\n",
      "[Epoch 0/100] [Batch 137/938] [D loss: 0.345576] [G loss: 1.704709]\n",
      "[Epoch 0/100] [Batch 138/938] [D loss: 0.342188] [G loss: 1.134355]\n",
      "[Epoch 0/100] [Batch 139/938] [D loss: 0.221035] [G loss: 2.067823]\n",
      "[Epoch 0/100] [Batch 140/938] [D loss: 0.265045] [G loss: 2.286976]\n",
      "[Epoch 0/100] [Batch 141/938] [D loss: 0.456109] [G loss: 0.893703]\n",
      "[Epoch 0/100] [Batch 142/938] [D loss: 0.465639] [G loss: 2.448907]\n",
      "[Epoch 0/100] [Batch 143/938] [D loss: 0.662333] [G loss: 0.484631]\n",
      "[Epoch 0/100] [Batch 144/938] [D loss: 0.546585] [G loss: 2.845185]\n",
      "[Epoch 0/100] [Batch 145/938] [D loss: 0.279377] [G loss: 1.442237]\n",
      "[Epoch 0/100] [Batch 146/938] [D loss: 0.349832] [G loss: 1.139609]\n",
      "[Epoch 0/100] [Batch 147/938] [D loss: 0.339666] [G loss: 2.740414]\n",
      "[Epoch 0/100] [Batch 148/938] [D loss: 0.289956] [G loss: 1.335486]\n",
      "[Epoch 0/100] [Batch 149/938] [D loss: 0.263853] [G loss: 1.483971]\n",
      "[Epoch 0/100] [Batch 150/938] [D loss: 0.391197] [G loss: 1.900174]\n",
      "[Epoch 0/100] [Batch 151/938] [D loss: 0.515874] [G loss: 0.746392]\n",
      "[Epoch 0/100] [Batch 152/938] [D loss: 0.545370] [G loss: 3.104953]\n",
      "[Epoch 0/100] [Batch 153/938] [D loss: 0.380119] [G loss: 1.004609]\n",
      "[Epoch 0/100] [Batch 154/938] [D loss: 0.257226] [G loss: 2.450495]\n",
      "[Epoch 0/100] [Batch 155/938] [D loss: 0.302557] [G loss: 2.138367]\n",
      "[Epoch 0/100] [Batch 156/938] [D loss: 0.127308] [G loss: 2.100606]\n",
      "[Epoch 0/100] [Batch 157/938] [D loss: 0.431275] [G loss: 0.806413]\n",
      "[Epoch 0/100] [Batch 158/938] [D loss: 0.451027] [G loss: 2.871758]\n",
      "[Epoch 0/100] [Batch 159/938] [D loss: 0.147100] [G loss: 2.490320]\n",
      "[Epoch 0/100] [Batch 160/938] [D loss: 0.206051] [G loss: 1.640610]\n",
      "[Epoch 0/100] [Batch 161/938] [D loss: 0.233841] [G loss: 1.682436]\n",
      "[Epoch 0/100] [Batch 162/938] [D loss: 0.293371] [G loss: 3.635234]\n",
      "[Epoch 0/100] [Batch 163/938] [D loss: 0.038691] [G loss: 3.285178]\n",
      "[Epoch 0/100] [Batch 164/938] [D loss: 0.795952] [G loss: 0.333990]\n",
      "[Epoch 0/100] [Batch 165/938] [D loss: 0.604169] [G loss: 6.083241]\n",
      "[Epoch 0/100] [Batch 166/938] [D loss: 0.140404] [G loss: 2.542402]\n",
      "[Epoch 0/100] [Batch 167/938] [D loss: 0.590583] [G loss: 0.516327]\n",
      "[Epoch 0/100] [Batch 168/938] [D loss: 0.665430] [G loss: 4.064223]\n",
      "[Epoch 0/100] [Batch 169/938] [D loss: 0.359488] [G loss: 0.951890]\n",
      "[Epoch 0/100] [Batch 170/938] [D loss: 0.218118] [G loss: 1.709036]\n",
      "[Epoch 0/100] [Batch 171/938] [D loss: 0.302405] [G loss: 2.108234]\n",
      "[Epoch 0/100] [Batch 172/938] [D loss: 0.340294] [G loss: 1.253353]\n",
      "[Epoch 0/100] [Batch 173/938] [D loss: 0.286651] [G loss: 2.050109]\n",
      "[Epoch 0/100] [Batch 174/938] [D loss: 0.331491] [G loss: 1.481215]\n",
      "[Epoch 0/100] [Batch 175/938] [D loss: 0.316302] [G loss: 1.494949]\n",
      "[Epoch 0/100] [Batch 176/938] [D loss: 0.355638] [G loss: 2.549468]\n",
      "[Epoch 0/100] [Batch 177/938] [D loss: 0.578138] [G loss: 0.517361]\n",
      "[Epoch 0/100] [Batch 178/938] [D loss: 0.629729] [G loss: 3.729131]\n",
      "[Epoch 0/100] [Batch 179/938] [D loss: 0.338590] [G loss: 1.128418]\n",
      "[Epoch 0/100] [Batch 180/938] [D loss: 0.300100] [G loss: 1.315752]\n",
      "[Epoch 0/100] [Batch 181/938] [D loss: 0.708919] [G loss: 2.761621]\n",
      "[Epoch 0/100] [Batch 182/938] [D loss: 0.827852] [G loss: 0.308052]\n",
      "[Epoch 0/100] [Batch 183/938] [D loss: 0.359043] [G loss: 2.952743]\n",
      "[Epoch 0/100] [Batch 184/938] [D loss: 0.176829] [G loss: 2.655923]\n",
      "[Epoch 0/100] [Batch 185/938] [D loss: 0.401969] [G loss: 0.829769]\n",
      "[Epoch 0/100] [Batch 186/938] [D loss: 0.422052] [G loss: 1.841242]\n",
      "[Epoch 0/100] [Batch 187/938] [D loss: 0.247187] [G loss: 2.806136]\n",
      "[Epoch 0/100] [Batch 188/938] [D loss: 0.415216] [G loss: 0.749953]\n",
      "[Epoch 0/100] [Batch 189/938] [D loss: 0.367441] [G loss: 2.177145]\n",
      "[Epoch 0/100] [Batch 190/938] [D loss: 0.076101] [G loss: 5.494210]\n",
      "[Epoch 0/100] [Batch 191/938] [D loss: 0.079137] [G loss: 2.830540]\n",
      "[Epoch 0/100] [Batch 192/938] [D loss: 0.153823] [G loss: 1.894928]\n",
      "[Epoch 0/100] [Batch 193/938] [D loss: 0.256719] [G loss: 1.316191]\n",
      "[Epoch 0/100] [Batch 194/938] [D loss: 0.460375] [G loss: 3.755423]\n",
      "[Epoch 0/100] [Batch 195/938] [D loss: 0.241121] [G loss: 1.284775]\n",
      "[Epoch 0/100] [Batch 196/938] [D loss: 0.159201] [G loss: 1.863004]\n",
      "[Epoch 0/100] [Batch 197/938] [D loss: 0.238551] [G loss: 1.611210]\n",
      "[Epoch 0/100] [Batch 198/938] [D loss: 0.362814] [G loss: 2.465330]\n",
      "[Epoch 0/100] [Batch 199/938] [D loss: 0.140279] [G loss: 1.972177]\n",
      "[Epoch 0/100] [Batch 200/938] [D loss: 0.034619] [G loss: 7.299914]\n",
      "[Epoch 0/100] [Batch 201/938] [D loss: 0.026045] [G loss: 4.616572]\n",
      "[Epoch 0/100] [Batch 202/938] [D loss: 0.295739] [G loss: 1.077286]\n",
      "[Epoch 0/100] [Batch 203/938] [D loss: 0.311993] [G loss: 5.457453]\n",
      "[Epoch 0/100] [Batch 204/938] [D loss: 0.117374] [G loss: 2.056415]\n",
      "[Epoch 0/100] [Batch 205/938] [D loss: 0.315054] [G loss: 1.112241]\n",
      "[Epoch 0/100] [Batch 206/938] [D loss: 0.675147] [G loss: 4.142715]\n",
      "[Epoch 0/100] [Batch 207/938] [D loss: 0.715738] [G loss: 0.357729]\n",
      "[Epoch 0/100] [Batch 208/938] [D loss: 0.542615] [G loss: 3.127392]\n",
      "[Epoch 0/100] [Batch 209/938] [D loss: 0.494343] [G loss: 0.726492]\n",
      "[Epoch 0/100] [Batch 210/938] [D loss: 0.339351] [G loss: 1.884628]\n",
      "[Epoch 0/100] [Batch 211/938] [D loss: 0.377632] [G loss: 1.671966]\n",
      "[Epoch 0/100] [Batch 212/938] [D loss: 0.420847] [G loss: 0.977996]\n",
      "[Epoch 0/100] [Batch 213/938] [D loss: 0.341299] [G loss: 2.044338]\n",
      "[Epoch 0/100] [Batch 214/938] [D loss: 0.279752] [G loss: 1.463156]\n",
      "[Epoch 0/100] [Batch 215/938] [D loss: 0.236641] [G loss: 1.936071]\n",
      "[Epoch 0/100] [Batch 216/938] [D loss: 0.254700] [G loss: 1.751583]\n",
      "[Epoch 0/100] [Batch 217/938] [D loss: 0.333943] [G loss: 1.242266]\n",
      "[Epoch 0/100] [Batch 218/938] [D loss: 0.381166] [G loss: 2.125454]\n",
      "[Epoch 0/100] [Batch 219/938] [D loss: 0.447230] [G loss: 0.756689]\n",
      "[Epoch 0/100] [Batch 220/938] [D loss: 0.490331] [G loss: 3.906590]\n",
      "[Epoch 0/100] [Batch 221/938] [D loss: 0.378480] [G loss: 1.047144]\n",
      "[Epoch 0/100] [Batch 222/938] [D loss: 0.203170] [G loss: 1.974447]\n",
      "[Epoch 0/100] [Batch 223/938] [D loss: 0.256397] [G loss: 2.161170]\n",
      "[Epoch 0/100] [Batch 224/938] [D loss: 0.309042] [G loss: 1.340129]\n",
      "[Epoch 0/100] [Batch 225/938] [D loss: 0.303514] [G loss: 1.643759]\n",
      "[Epoch 0/100] [Batch 226/938] [D loss: 0.328737] [G loss: 1.423296]\n",
      "[Epoch 0/100] [Batch 227/938] [D loss: 0.301320] [G loss: 2.003672]\n",
      "[Epoch 0/100] [Batch 228/938] [D loss: 0.278767] [G loss: 1.216082]\n",
      "[Epoch 0/100] [Batch 229/938] [D loss: 0.336072] [G loss: 2.946020]\n",
      "[Epoch 0/100] [Batch 230/938] [D loss: 0.567386] [G loss: 0.583930]\n",
      "[Epoch 0/100] [Batch 231/938] [D loss: 0.690462] [G loss: 4.533415]\n",
      "[Epoch 0/100] [Batch 232/938] [D loss: 0.779156] [G loss: 0.350498]\n",
      "[Epoch 0/100] [Batch 233/938] [D loss: 0.327750] [G loss: 3.332106]\n",
      "[Epoch 0/100] [Batch 234/938] [D loss: 0.230779] [G loss: 2.111330]\n",
      "[Epoch 0/100] [Batch 235/938] [D loss: 0.593700] [G loss: 0.580986]\n",
      "[Epoch 0/100] [Batch 236/938] [D loss: 0.405521] [G loss: 3.224241]\n",
      "[Epoch 0/100] [Batch 237/938] [D loss: 0.376563] [G loss: 0.929465]\n",
      "[Epoch 0/100] [Batch 238/938] [D loss: 0.340365] [G loss: 1.912930]\n",
      "[Epoch 0/100] [Batch 239/938] [D loss: 0.308219] [G loss: 1.417548]\n",
      "[Epoch 0/100] [Batch 240/938] [D loss: 0.361517] [G loss: 1.939041]\n",
      "[Epoch 0/100] [Batch 241/938] [D loss: 0.208559] [G loss: 1.617198]\n",
      "[Epoch 0/100] [Batch 242/938] [D loss: 0.343866] [G loss: 2.297615]\n",
      "[Epoch 0/100] [Batch 243/938] [D loss: 0.385128] [G loss: 0.861257]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/100] [Batch 244/938] [D loss: 0.239181] [G loss: 3.298580]\n",
      "[Epoch 0/100] [Batch 245/938] [D loss: 0.350754] [G loss: 1.128303]\n",
      "[Epoch 0/100] [Batch 246/938] [D loss: 0.339922] [G loss: 2.196011]\n",
      "[Epoch 0/100] [Batch 247/938] [D loss: 0.269330] [G loss: 1.353807]\n",
      "[Epoch 0/100] [Batch 248/938] [D loss: 0.255564] [G loss: 2.422520]\n",
      "[Epoch 0/100] [Batch 249/938] [D loss: 0.294264] [G loss: 1.620167]\n",
      "[Epoch 0/100] [Batch 250/938] [D loss: 0.271381] [G loss: 1.784716]\n",
      "[Epoch 0/100] [Batch 251/938] [D loss: 0.389287] [G loss: 1.099103]\n",
      "[Epoch 0/100] [Batch 252/938] [D loss: 0.668283] [G loss: 3.131779]\n",
      "[Epoch 0/100] [Batch 253/938] [D loss: 1.058387] [G loss: 0.166339]\n",
      "[Epoch 0/100] [Batch 254/938] [D loss: 0.803083] [G loss: 5.202615]\n",
      "[Epoch 0/100] [Batch 255/938] [D loss: 0.207035] [G loss: 2.208984]\n",
      "[Epoch 0/100] [Batch 256/938] [D loss: 0.587070] [G loss: 0.509594]\n",
      "[Epoch 0/100] [Batch 257/938] [D loss: 0.360476] [G loss: 2.601809]\n",
      "[Epoch 0/100] [Batch 258/938] [D loss: 0.470898] [G loss: 1.538689]\n",
      "[Epoch 0/100] [Batch 259/938] [D loss: 0.673904] [G loss: 0.500117]\n",
      "[Epoch 0/100] [Batch 260/938] [D loss: 0.581417] [G loss: 1.661429]\n",
      "[Epoch 0/100] [Batch 261/938] [D loss: 0.374131] [G loss: 1.341045]\n",
      "[Epoch 0/100] [Batch 262/938] [D loss: 0.537306] [G loss: 0.968865]\n",
      "[Epoch 0/100] [Batch 263/938] [D loss: 0.326613] [G loss: 2.038116]\n",
      "[Epoch 0/100] [Batch 264/938] [D loss: 0.265363] [G loss: 1.613212]\n",
      "[Epoch 0/100] [Batch 265/938] [D loss: 0.275557] [G loss: 1.599854]\n",
      "[Epoch 0/100] [Batch 266/938] [D loss: 0.344701] [G loss: 1.949275]\n",
      "[Epoch 0/100] [Batch 267/938] [D loss: 0.394319] [G loss: 0.917017]\n",
      "[Epoch 0/100] [Batch 268/938] [D loss: 0.488637] [G loss: 2.174599]\n",
      "[Epoch 0/100] [Batch 269/938] [D loss: 0.497373] [G loss: 0.678640]\n",
      "[Epoch 0/100] [Batch 270/938] [D loss: 0.465386] [G loss: 2.592702]\n",
      "[Epoch 0/100] [Batch 271/938] [D loss: 0.419898] [G loss: 0.933588]\n",
      "[Epoch 0/100] [Batch 272/938] [D loss: 0.336645] [G loss: 1.675856]\n",
      "[Epoch 0/100] [Batch 273/938] [D loss: 0.341470] [G loss: 1.442648]\n",
      "[Epoch 0/100] [Batch 274/938] [D loss: 0.329340] [G loss: 1.544525]\n",
      "[Epoch 0/100] [Batch 275/938] [D loss: 0.298384] [G loss: 1.899584]\n",
      "[Epoch 0/100] [Batch 276/938] [D loss: 0.298127] [G loss: 1.155323]\n",
      "[Epoch 0/100] [Batch 277/938] [D loss: 0.529795] [G loss: 3.047097]\n",
      "[Epoch 0/100] [Batch 278/938] [D loss: 0.484493] [G loss: 0.655175]\n",
      "[Epoch 0/100] [Batch 279/938] [D loss: 0.148550] [G loss: 2.748428]\n",
      "[Epoch 0/100] [Batch 280/938] [D loss: 0.257588] [G loss: 4.270927]\n",
      "[Epoch 0/100] [Batch 281/938] [D loss: 0.396091] [G loss: 0.792331]\n",
      "[Epoch 0/100] [Batch 282/938] [D loss: 0.208947] [G loss: 2.962996]\n",
      "[Epoch 0/100] [Batch 283/938] [D loss: 0.248180] [G loss: 2.523644]\n",
      "[Epoch 0/100] [Batch 284/938] [D loss: 0.292958] [G loss: 1.116938]\n",
      "[Epoch 0/100] [Batch 285/938] [D loss: 0.172266] [G loss: 2.633345]\n",
      "[Epoch 0/100] [Batch 286/938] [D loss: 0.228303] [G loss: 2.423632]\n",
      "[Epoch 0/100] [Batch 287/938] [D loss: 0.080100] [G loss: 2.887688]\n",
      "[Epoch 0/100] [Batch 288/938] [D loss: 0.045051] [G loss: 4.055168]\n",
      "[Epoch 0/100] [Batch 289/938] [D loss: 0.278571] [G loss: 1.107309]\n",
      "[Epoch 0/100] [Batch 290/938] [D loss: 0.492118] [G loss: 4.458721]\n",
      "[Epoch 0/100] [Batch 291/938] [D loss: 0.562355] [G loss: 0.534102]\n",
      "[Epoch 0/100] [Batch 292/938] [D loss: 0.435095] [G loss: 4.685845]\n",
      "[Epoch 0/100] [Batch 293/938] [D loss: 0.223295] [G loss: 1.424597]\n",
      "[Epoch 0/100] [Batch 294/938] [D loss: 0.199743] [G loss: 1.754307]\n",
      "[Epoch 0/100] [Batch 295/938] [D loss: 0.252393] [G loss: 1.999602]\n",
      "[Epoch 0/100] [Batch 296/938] [D loss: 0.327519] [G loss: 1.394044]\n",
      "[Epoch 0/100] [Batch 297/938] [D loss: 0.408848] [G loss: 1.898195]\n",
      "[Epoch 0/100] [Batch 298/938] [D loss: 0.628912] [G loss: 0.458395]\n",
      "[Epoch 0/100] [Batch 299/938] [D loss: 0.843122] [G loss: 4.021286]\n",
      "[Epoch 0/100] [Batch 300/938] [D loss: 0.521177] [G loss: 0.659885]\n",
      "[Epoch 0/100] [Batch 301/938] [D loss: 0.229523] [G loss: 2.668377]\n",
      "[Epoch 0/100] [Batch 302/938] [D loss: 0.281516] [G loss: 2.395277]\n",
      "[Epoch 0/100] [Batch 303/938] [D loss: 0.383752] [G loss: 0.957397]\n",
      "[Epoch 0/100] [Batch 304/938] [D loss: 0.368833] [G loss: 2.399053]\n",
      "[Epoch 0/100] [Batch 305/938] [D loss: 0.413947] [G loss: 0.965387]\n",
      "[Epoch 0/100] [Batch 306/938] [D loss: 0.488257] [G loss: 1.517732]\n",
      "[Epoch 0/100] [Batch 307/938] [D loss: 0.286173] [G loss: 1.902979]\n",
      "[Epoch 0/100] [Batch 308/938] [D loss: 0.377893] [G loss: 1.004036]\n",
      "[Epoch 0/100] [Batch 309/938] [D loss: 0.455179] [G loss: 3.569959]\n",
      "[Epoch 0/100] [Batch 310/938] [D loss: 0.404491] [G loss: 0.802970]\n",
      "[Epoch 0/100] [Batch 311/938] [D loss: 0.233863] [G loss: 2.466969]\n",
      "[Epoch 0/100] [Batch 312/938] [D loss: 0.377022] [G loss: 1.625709]\n",
      "[Epoch 0/100] [Batch 313/938] [D loss: 0.448875] [G loss: 0.767668]\n",
      "[Epoch 0/100] [Batch 314/938] [D loss: 0.465715] [G loss: 2.955675]\n",
      "[Epoch 0/100] [Batch 315/938] [D loss: 0.447132] [G loss: 0.864887]\n",
      "[Epoch 0/100] [Batch 316/938] [D loss: 0.211150] [G loss: 2.273721]\n",
      "[Epoch 0/100] [Batch 317/938] [D loss: 0.258342] [G loss: 2.082442]\n",
      "[Epoch 0/100] [Batch 318/938] [D loss: 0.215359] [G loss: 1.610725]\n",
      "[Epoch 0/100] [Batch 319/938] [D loss: 0.245682] [G loss: 2.121480]\n",
      "[Epoch 0/100] [Batch 320/938] [D loss: 0.199937] [G loss: 1.547273]\n",
      "[Epoch 0/100] [Batch 321/938] [D loss: 0.231204] [G loss: 2.239121]\n",
      "[Epoch 0/100] [Batch 322/938] [D loss: 0.093822] [G loss: 4.062674]\n",
      "[Epoch 0/100] [Batch 323/938] [D loss: 0.080434] [G loss: 2.429385]\n",
      "[Epoch 0/100] [Batch 324/938] [D loss: 0.105895] [G loss: 2.028871]\n",
      "[Epoch 0/100] [Batch 325/938] [D loss: 0.154111] [G loss: 2.902743]\n",
      "[Epoch 0/100] [Batch 326/938] [D loss: 0.085518] [G loss: 3.142128]\n",
      "[Epoch 0/100] [Batch 327/938] [D loss: 0.073961] [G loss: 2.608311]\n",
      "[Epoch 0/100] [Batch 328/938] [D loss: 0.140531] [G loss: 1.791656]\n",
      "[Epoch 0/100] [Batch 329/938] [D loss: 0.258660] [G loss: 4.573220]\n",
      "[Epoch 0/100] [Batch 330/938] [D loss: 0.204822] [G loss: 1.576420]\n",
      "[Epoch 0/100] [Batch 331/938] [D loss: 0.069573] [G loss: 3.352050]\n",
      "[Epoch 0/100] [Batch 332/938] [D loss: 0.043020] [G loss: 4.316420]\n",
      "[Epoch 0/100] [Batch 333/938] [D loss: 0.054666] [G loss: 3.627303]\n",
      "[Epoch 0/100] [Batch 334/938] [D loss: 0.071753] [G loss: 2.688839]\n",
      "[Epoch 0/100] [Batch 335/938] [D loss: 0.028050] [G loss: 4.793349]\n",
      "[Epoch 0/100] [Batch 336/938] [D loss: 0.044277] [G loss: 4.917108]\n",
      "[Epoch 0/100] [Batch 337/938] [D loss: 0.015360] [G loss: 4.534530]\n",
      "[Epoch 0/100] [Batch 338/938] [D loss: 0.017547] [G loss: 4.253558]\n",
      "[Epoch 0/100] [Batch 339/938] [D loss: 0.016327] [G loss: 4.211822]\n",
      "[Epoch 0/100] [Batch 340/938] [D loss: 0.027449] [G loss: 3.627423]\n",
      "[Epoch 0/100] [Batch 341/938] [D loss: 0.037617] [G loss: 3.642824]\n",
      "[Epoch 0/100] [Batch 342/938] [D loss: 0.024000] [G loss: 4.394250]\n",
      "[Epoch 0/100] [Batch 343/938] [D loss: 0.038015] [G loss: 4.139321]\n",
      "[Epoch 0/100] [Batch 344/938] [D loss: 0.024943] [G loss: 4.408600]\n",
      "[Epoch 0/100] [Batch 345/938] [D loss: 0.085218] [G loss: 2.992983]\n",
      "[Epoch 0/100] [Batch 346/938] [D loss: 0.025687] [G loss: 5.118862]\n",
      "[Epoch 0/100] [Batch 347/938] [D loss: 0.028233] [G loss: 4.420627]\n",
      "[Epoch 0/100] [Batch 348/938] [D loss: 0.016928] [G loss: 4.831810]\n",
      "[Epoch 0/100] [Batch 349/938] [D loss: 0.023915] [G loss: 4.329544]\n",
      "[Epoch 0/100] [Batch 350/938] [D loss: 0.050230] [G loss: 3.167470]\n",
      "[Epoch 0/100] [Batch 351/938] [D loss: 0.011986] [G loss: 5.853229]\n",
      "[Epoch 0/100] [Batch 352/938] [D loss: 0.014057] [G loss: 5.625062]\n",
      "[Epoch 0/100] [Batch 353/938] [D loss: 0.039507] [G loss: 3.232578]\n",
      "[Epoch 0/100] [Batch 354/938] [D loss: 0.076026] [G loss: 2.436532]\n",
      "[Epoch 0/100] [Batch 355/938] [D loss: 0.026421] [G loss: 5.305013]\n",
      "[Epoch 0/100] [Batch 356/938] [D loss: 0.027455] [G loss: 4.620763]\n",
      "[Epoch 0/100] [Batch 357/938] [D loss: 0.029769] [G loss: 3.688906]\n",
      "[Epoch 0/100] [Batch 358/938] [D loss: 0.017321] [G loss: 4.210455]\n",
      "[Epoch 0/100] [Batch 359/938] [D loss: 0.017115] [G loss: 4.428089]\n",
      "[Epoch 0/100] [Batch 360/938] [D loss: 0.014109] [G loss: 4.338659]\n",
      "[Epoch 0/100] [Batch 361/938] [D loss: 0.012283] [G loss: 4.368766]\n",
      "[Epoch 0/100] [Batch 362/938] [D loss: 0.012248] [G loss: 4.936220]\n",
      "[Epoch 0/100] [Batch 363/938] [D loss: 0.010397] [G loss: 5.081221]\n",
      "[Epoch 0/100] [Batch 364/938] [D loss: 0.007802] [G loss: 5.035797]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/100] [Batch 365/938] [D loss: 0.023852] [G loss: 3.519029]\n",
      "[Epoch 0/100] [Batch 366/938] [D loss: 0.013827] [G loss: 4.641695]\n",
      "[Epoch 0/100] [Batch 367/938] [D loss: 0.012683] [G loss: 4.737578]\n",
      "[Epoch 0/100] [Batch 368/938] [D loss: 0.026014] [G loss: 3.741496]\n",
      "[Epoch 0/100] [Batch 369/938] [D loss: 0.031569] [G loss: 3.480933]\n",
      "[Epoch 0/100] [Batch 370/938] [D loss: 0.012521] [G loss: 5.241048]\n",
      "[Epoch 0/100] [Batch 371/938] [D loss: 0.031071] [G loss: 5.712213]\n",
      "[Epoch 0/100] [Batch 372/938] [D loss: 0.009004] [G loss: 4.729275]\n",
      "[Epoch 0/100] [Batch 373/938] [D loss: 0.007625] [G loss: 5.148946]\n",
      "[Epoch 0/100] [Batch 374/938] [D loss: 0.017984] [G loss: 3.777400]\n",
      "[Epoch 0/100] [Batch 375/938] [D loss: 0.012396] [G loss: 4.258850]\n",
      "[Epoch 0/100] [Batch 376/938] [D loss: 0.005411] [G loss: 5.627436]\n",
      "[Epoch 0/100] [Batch 377/938] [D loss: 0.010040] [G loss: 5.553759]\n",
      "[Epoch 0/100] [Batch 378/938] [D loss: 0.006923] [G loss: 5.161687]\n",
      "[Epoch 0/100] [Batch 379/938] [D loss: 0.008732] [G loss: 4.678671]\n",
      "[Epoch 0/100] [Batch 380/938] [D loss: 0.008589] [G loss: 4.628282]\n",
      "[Epoch 0/100] [Batch 381/938] [D loss: 0.009675] [G loss: 4.741693]\n",
      "[Epoch 0/100] [Batch 382/938] [D loss: 0.010431] [G loss: 4.494742]\n",
      "[Epoch 0/100] [Batch 383/938] [D loss: 0.011177] [G loss: 4.294566]\n",
      "[Epoch 0/100] [Batch 384/938] [D loss: 0.007702] [G loss: 4.556141]\n",
      "[Epoch 0/100] [Batch 385/938] [D loss: 0.009011] [G loss: 4.511165]\n",
      "[Epoch 0/100] [Batch 386/938] [D loss: 0.008895] [G loss: 4.689629]\n",
      "[Epoch 0/100] [Batch 387/938] [D loss: 0.007639] [G loss: 4.951270]\n",
      "[Epoch 0/100] [Batch 388/938] [D loss: 0.004750] [G loss: 5.535569]\n",
      "[Epoch 0/100] [Batch 389/938] [D loss: 0.004299] [G loss: 5.405513]\n",
      "[Epoch 0/100] [Batch 390/938] [D loss: 0.008611] [G loss: 4.379315]\n",
      "[Epoch 0/100] [Batch 391/938] [D loss: 0.004689] [G loss: 5.125898]\n",
      "[Epoch 0/100] [Batch 392/938] [D loss: 0.004154] [G loss: 5.386940]\n",
      "[Epoch 0/100] [Batch 393/938] [D loss: 0.003925] [G loss: 5.257994]\n",
      "[Epoch 0/100] [Batch 394/938] [D loss: 0.007170] [G loss: 5.043957]\n",
      "[Epoch 0/100] [Batch 395/938] [D loss: 0.008689] [G loss: 4.661711]\n",
      "[Epoch 0/100] [Batch 396/938] [D loss: 0.005211] [G loss: 5.323222]\n",
      "[Epoch 0/100] [Batch 397/938] [D loss: 0.002192] [G loss: 6.008694]\n",
      "[Epoch 0/100] [Batch 398/938] [D loss: 0.002637] [G loss: 6.144691]\n",
      "[Epoch 0/100] [Batch 399/938] [D loss: 0.004481] [G loss: 5.171360]\n",
      "[Epoch 0/100] [Batch 400/938] [D loss: 0.003811] [G loss: 5.435253]\n",
      "[Epoch 0/100] [Batch 401/938] [D loss: 0.002760] [G loss: 5.700998]\n",
      "[Epoch 0/100] [Batch 402/938] [D loss: 0.002350] [G loss: 5.902936]\n",
      "[Epoch 0/100] [Batch 403/938] [D loss: 0.002963] [G loss: 6.040071]\n",
      "[Epoch 0/100] [Batch 404/938] [D loss: 0.002493] [G loss: 5.852528]\n",
      "[Epoch 0/100] [Batch 405/938] [D loss: 0.003470] [G loss: 5.630816]\n",
      "[Epoch 0/100] [Batch 406/938] [D loss: 0.003442] [G loss: 5.509769]\n",
      "[Epoch 0/100] [Batch 407/938] [D loss: 0.001868] [G loss: 6.103928]\n",
      "[Epoch 0/100] [Batch 408/938] [D loss: 0.002581] [G loss: 5.921870]\n",
      "[Epoch 0/100] [Batch 409/938] [D loss: 0.003033] [G loss: 5.835140]\n",
      "[Epoch 0/100] [Batch 410/938] [D loss: 0.002905] [G loss: 5.630079]\n",
      "[Epoch 0/100] [Batch 411/938] [D loss: 0.011897] [G loss: 4.896713]\n",
      "[Epoch 0/100] [Batch 412/938] [D loss: 0.003604] [G loss: 5.468448]\n",
      "[Epoch 0/100] [Batch 413/938] [D loss: 0.002920] [G loss: 5.777114]\n",
      "[Epoch 0/100] [Batch 414/938] [D loss: 0.001976] [G loss: 6.214520]\n",
      "[Epoch 0/100] [Batch 415/938] [D loss: 0.002423] [G loss: 6.851641]\n",
      "[Epoch 0/100] [Batch 416/938] [D loss: 0.001272] [G loss: 7.178791]\n",
      "[Epoch 0/100] [Batch 417/938] [D loss: 0.001154] [G loss: 7.363602]\n",
      "[Epoch 0/100] [Batch 418/938] [D loss: 0.002486] [G loss: 6.117548]\n",
      "[Epoch 0/100] [Batch 419/938] [D loss: 0.006387] [G loss: 4.690687]\n",
      "[Epoch 0/100] [Batch 420/938] [D loss: 0.002078] [G loss: 5.891721]\n",
      "[Epoch 0/100] [Batch 421/938] [D loss: 0.001433] [G loss: 6.778212]\n",
      "[Epoch 0/100] [Batch 422/938] [D loss: 0.001325] [G loss: 7.061682]\n",
      "[Epoch 0/100] [Batch 423/938] [D loss: 0.001267] [G loss: 7.087087]\n",
      "[Epoch 0/100] [Batch 424/938] [D loss: 0.001920] [G loss: 7.304675]\n",
      "[Epoch 0/100] [Batch 425/938] [D loss: 0.001522] [G loss: 6.332083]\n",
      "[Epoch 0/100] [Batch 426/938] [D loss: 0.002204] [G loss: 6.237154]\n",
      "[Epoch 0/100] [Batch 427/938] [D loss: 0.001943] [G loss: 6.379663]\n",
      "[Epoch 0/100] [Batch 428/938] [D loss: 0.003575] [G loss: 6.082157]\n",
      "[Epoch 0/100] [Batch 429/938] [D loss: 0.002750] [G loss: 6.179605]\n",
      "[Epoch 0/100] [Batch 430/938] [D loss: 0.003379] [G loss: 8.029695]\n",
      "[Epoch 0/100] [Batch 431/938] [D loss: 0.005155] [G loss: 6.558367]\n",
      "[Epoch 0/100] [Batch 432/938] [D loss: 0.002159] [G loss: 6.030681]\n",
      "[Epoch 0/100] [Batch 433/938] [D loss: 0.004860] [G loss: 5.077558]\n",
      "[Epoch 0/100] [Batch 434/938] [D loss: 0.002561] [G loss: 5.759012]\n",
      "[Epoch 0/100] [Batch 435/938] [D loss: 0.003267] [G loss: 5.763345]\n",
      "[Epoch 0/100] [Batch 436/938] [D loss: 0.000901] [G loss: 7.957856]\n",
      "[Epoch 0/100] [Batch 437/938] [D loss: 0.001963] [G loss: 6.719651]\n",
      "[Epoch 0/100] [Batch 438/938] [D loss: 0.002055] [G loss: 6.183010]\n",
      "[Epoch 0/100] [Batch 439/938] [D loss: 0.004590] [G loss: 5.294270]\n",
      "[Epoch 0/100] [Batch 440/938] [D loss: 0.003206] [G loss: 5.752173]\n",
      "[Epoch 0/100] [Batch 441/938] [D loss: 0.001484] [G loss: 6.661579]\n",
      "[Epoch 0/100] [Batch 442/938] [D loss: 0.001636] [G loss: 6.993202]\n",
      "[Epoch 0/100] [Batch 443/938] [D loss: 0.001847] [G loss: 6.855014]\n",
      "[Epoch 0/100] [Batch 444/938] [D loss: 0.003402] [G loss: 5.525592]\n",
      "[Epoch 0/100] [Batch 445/938] [D loss: 0.004215] [G loss: 5.203620]\n",
      "[Epoch 0/100] [Batch 446/938] [D loss: 0.003098] [G loss: 5.948026]\n",
      "[Epoch 0/100] [Batch 447/938] [D loss: 0.002351] [G loss: 7.396801]\n",
      "[Epoch 0/100] [Batch 448/938] [D loss: 0.003237] [G loss: 5.760199]\n",
      "[Epoch 0/100] [Batch 449/938] [D loss: 0.004803] [G loss: 5.042960]\n",
      "[Epoch 0/100] [Batch 450/938] [D loss: 0.003741] [G loss: 5.655175]\n",
      "[Epoch 0/100] [Batch 451/938] [D loss: 0.002288] [G loss: 8.112131]\n",
      "[Epoch 0/100] [Batch 452/938] [D loss: 0.001941] [G loss: 6.500531]\n",
      "[Epoch 0/100] [Batch 453/938] [D loss: 0.003502] [G loss: 5.304794]\n",
      "[Epoch 0/100] [Batch 454/938] [D loss: 0.003810] [G loss: 5.918307]\n",
      "[Epoch 0/100] [Batch 455/938] [D loss: 0.001917] [G loss: 7.770155]\n",
      "[Epoch 0/100] [Batch 456/938] [D loss: 0.003092] [G loss: 5.558143]\n",
      "[Epoch 0/100] [Batch 457/938] [D loss: 0.002559] [G loss: 6.434780]\n",
      "[Epoch 0/100] [Batch 458/938] [D loss: 0.001757] [G loss: 7.223790]\n",
      "[Epoch 0/100] [Batch 459/938] [D loss: 0.016979] [G loss: 3.535712]\n",
      "[Epoch 0/100] [Batch 460/938] [D loss: 0.003365] [G loss: 7.995393]\n",
      "[Epoch 0/100] [Batch 461/938] [D loss: 0.005410] [G loss: 7.905400]\n",
      "[Epoch 0/100] [Batch 462/938] [D loss: 0.011157] [G loss: 6.283058]\n",
      "[Epoch 0/100] [Batch 463/938] [D loss: 0.005711] [G loss: 4.848006]\n",
      "[Epoch 0/100] [Batch 464/938] [D loss: 0.001558] [G loss: 8.904865]\n",
      "[Epoch 0/100] [Batch 465/938] [D loss: 0.001768] [G loss: 6.677553]\n",
      "[Epoch 0/100] [Batch 466/938] [D loss: 0.001485] [G loss: 6.803986]\n",
      "[Epoch 0/100] [Batch 467/938] [D loss: 0.000718] [G loss: 8.320482]\n",
      "[Epoch 0/100] [Batch 468/938] [D loss: 0.023307] [G loss: 3.276398]\n",
      "[Epoch 0/100] [Batch 469/938] [D loss: 0.015050] [G loss: 11.218082]\n",
      "[Epoch 0/100] [Batch 470/938] [D loss: 0.005832] [G loss: 10.869925]\n",
      "[Epoch 0/100] [Batch 471/938] [D loss: 0.002343] [G loss: 10.403097]\n",
      "[Epoch 0/100] [Batch 472/938] [D loss: 0.000518] [G loss: 11.210609]\n",
      "[Epoch 0/100] [Batch 473/938] [D loss: 0.015874] [G loss: 11.656107]\n",
      "[Epoch 0/100] [Batch 474/938] [D loss: 0.000835] [G loss: 6.856311]\n",
      "[Epoch 0/100] [Batch 475/938] [D loss: 0.001094] [G loss: 6.594155]\n",
      "[Epoch 0/100] [Batch 476/938] [D loss: 0.000367] [G loss: 8.339985]\n",
      "[Epoch 0/100] [Batch 477/938] [D loss: 0.000566] [G loss: 8.757374]\n",
      "[Epoch 0/100] [Batch 478/938] [D loss: 0.071132] [G loss: 2.186895]\n",
      "[Epoch 0/100] [Batch 479/938] [D loss: 0.599620] [G loss: 19.337305]\n",
      "[Epoch 0/100] [Batch 480/938] [D loss: 1.294133] [G loss: 0.091966]\n",
      "[Epoch 0/100] [Batch 481/938] [D loss: 3.440255] [G loss: 25.205259]\n",
      "[Epoch 0/100] [Batch 482/938] [D loss: 0.221585] [G loss: 19.412420]\n",
      "[Epoch 0/100] [Batch 483/938] [D loss: 0.003568] [G loss: 11.105607]\n",
      "[Epoch 0/100] [Batch 484/938] [D loss: 0.163028] [G loss: 1.813073]\n",
      "[Epoch 0/100] [Batch 485/938] [D loss: 0.070648] [G loss: 2.666144]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/100] [Batch 486/938] [D loss: 0.060687] [G loss: 3.691422]\n",
      "[Epoch 0/100] [Batch 487/938] [D loss: 0.116624] [G loss: 3.005075]\n",
      "[Epoch 0/100] [Batch 488/938] [D loss: 0.156144] [G loss: 2.402021]\n",
      "[Epoch 0/100] [Batch 489/938] [D loss: 0.093909] [G loss: 3.531733]\n",
      "[Epoch 0/100] [Batch 490/938] [D loss: 0.089712] [G loss: 2.410097]\n",
      "[Epoch 0/100] [Batch 491/938] [D loss: 0.055490] [G loss: 3.621277]\n",
      "[Epoch 0/100] [Batch 492/938] [D loss: 0.100624] [G loss: 2.014363]\n",
      "[Epoch 0/100] [Batch 493/938] [D loss: 0.044233] [G loss: 3.896205]\n",
      "[Epoch 0/100] [Batch 494/938] [D loss: 0.187919] [G loss: 1.826701]\n",
      "[Epoch 0/100] [Batch 495/938] [D loss: 0.090956] [G loss: 4.803893]\n",
      "[Epoch 0/100] [Batch 496/938] [D loss: 0.148950] [G loss: 2.027843]\n",
      "[Epoch 0/100] [Batch 497/938] [D loss: 0.062015] [G loss: 3.915970]\n",
      "[Epoch 0/100] [Batch 498/938] [D loss: 0.066082] [G loss: 4.068636]\n",
      "[Epoch 0/100] [Batch 499/938] [D loss: 0.058782] [G loss: 3.262176]\n",
      "[Epoch 0/100] [Batch 500/938] [D loss: 0.074888] [G loss: 3.197276]\n",
      "[Epoch 0/100] [Batch 501/938] [D loss: 0.028891] [G loss: 4.864070]\n",
      "[Epoch 0/100] [Batch 502/938] [D loss: 0.032081] [G loss: 3.971834]\n",
      "[Epoch 0/100] [Batch 503/938] [D loss: 0.064917] [G loss: 3.017587]\n",
      "[Epoch 0/100] [Batch 504/938] [D loss: 0.043698] [G loss: 3.339589]\n",
      "[Epoch 0/100] [Batch 505/938] [D loss: 0.109508] [G loss: 2.823049]\n",
      "[Epoch 0/100] [Batch 506/938] [D loss: 0.140535] [G loss: 2.287979]\n",
      "[Epoch 0/100] [Batch 507/938] [D loss: 0.158665] [G loss: 4.443946]\n",
      "[Epoch 0/100] [Batch 508/938] [D loss: 0.159888] [G loss: 2.141419]\n",
      "[Epoch 0/100] [Batch 509/938] [D loss: 0.061509] [G loss: 4.699581]\n",
      "[Epoch 0/100] [Batch 510/938] [D loss: 0.049707] [G loss: 4.357571]\n",
      "[Epoch 0/100] [Batch 511/938] [D loss: 0.045333] [G loss: 3.425258]\n",
      "[Epoch 0/100] [Batch 512/938] [D loss: 0.056624] [G loss: 2.897634]\n",
      "[Epoch 0/100] [Batch 513/938] [D loss: 0.035898] [G loss: 4.792071]\n",
      "[Epoch 0/100] [Batch 514/938] [D loss: 0.048025] [G loss: 3.668210]\n",
      "[Epoch 0/100] [Batch 515/938] [D loss: 0.045394] [G loss: 3.327213]\n",
      "[Epoch 0/100] [Batch 516/938] [D loss: 0.052546] [G loss: 3.278003]\n",
      "[Epoch 0/100] [Batch 517/938] [D loss: 0.027355] [G loss: 4.046454]\n",
      "[Epoch 0/100] [Batch 518/938] [D loss: 0.051890] [G loss: 3.661812]\n",
      "[Epoch 0/100] [Batch 519/938] [D loss: 0.044758] [G loss: 3.636209]\n",
      "[Epoch 0/100] [Batch 520/938] [D loss: 0.032888] [G loss: 4.175369]\n",
      "[Epoch 0/100] [Batch 521/938] [D loss: 0.024693] [G loss: 4.094885]\n",
      "[Epoch 0/100] [Batch 522/938] [D loss: 0.019330] [G loss: 4.233257]\n",
      "[Epoch 0/100] [Batch 523/938] [D loss: 0.027935] [G loss: 4.234706]\n",
      "[Epoch 0/100] [Batch 524/938] [D loss: 0.021993] [G loss: 4.105955]\n",
      "[Epoch 0/100] [Batch 525/938] [D loss: 0.032037] [G loss: 3.400898]\n",
      "[Epoch 0/100] [Batch 526/938] [D loss: 0.021957] [G loss: 4.038867]\n",
      "[Epoch 0/100] [Batch 527/938] [D loss: 0.029209] [G loss: 4.101205]\n",
      "[Epoch 0/100] [Batch 528/938] [D loss: 0.039077] [G loss: 4.097461]\n",
      "[Epoch 0/100] [Batch 529/938] [D loss: 0.070834] [G loss: 2.523626]\n",
      "[Epoch 0/100] [Batch 530/938] [D loss: 0.027456] [G loss: 4.809370]\n",
      "[Epoch 0/100] [Batch 531/938] [D loss: 0.034647] [G loss: 4.539188]\n",
      "[Epoch 0/100] [Batch 532/938] [D loss: 0.032981] [G loss: 3.822904]\n",
      "[Epoch 0/100] [Batch 533/938] [D loss: 0.045317] [G loss: 3.270494]\n",
      "[Epoch 0/100] [Batch 534/938] [D loss: 0.045780] [G loss: 3.286964]\n",
      "[Epoch 0/100] [Batch 535/938] [D loss: 0.027513] [G loss: 4.208936]\n",
      "[Epoch 0/100] [Batch 536/938] [D loss: 0.050247] [G loss: 4.258360]\n",
      "[Epoch 0/100] [Batch 537/938] [D loss: 0.084334] [G loss: 2.519163]\n",
      "[Epoch 0/100] [Batch 538/938] [D loss: 0.057822] [G loss: 3.684432]\n",
      "[Epoch 0/100] [Batch 539/938] [D loss: 0.081809] [G loss: 4.331667]\n",
      "[Epoch 0/100] [Batch 540/938] [D loss: 0.139655] [G loss: 1.725058]\n",
      "[Epoch 0/100] [Batch 541/938] [D loss: 0.118313] [G loss: 6.918624]\n",
      "[Epoch 0/100] [Batch 542/938] [D loss: 0.037911] [G loss: 5.020024]\n",
      "[Epoch 0/100] [Batch 543/938] [D loss: 0.128477] [G loss: 1.943582]\n",
      "[Epoch 0/100] [Batch 544/938] [D loss: 0.057670] [G loss: 5.188498]\n",
      "[Epoch 0/100] [Batch 545/938] [D loss: 0.071677] [G loss: 4.545835]\n",
      "[Epoch 0/100] [Batch 546/938] [D loss: 0.126287] [G loss: 1.943927]\n",
      "[Epoch 0/100] [Batch 547/938] [D loss: 0.073820] [G loss: 4.984033]\n",
      "[Epoch 0/100] [Batch 548/938] [D loss: 0.078027] [G loss: 3.483016]\n",
      "[Epoch 0/100] [Batch 549/938] [D loss: 0.097898] [G loss: 2.160904]\n",
      "[Epoch 0/100] [Batch 550/938] [D loss: 0.103869] [G loss: 4.448241]\n",
      "[Epoch 0/100] [Batch 551/938] [D loss: 0.095979] [G loss: 2.230709]\n",
      "[Epoch 0/100] [Batch 552/938] [D loss: 0.053704] [G loss: 3.765830]\n",
      "[Epoch 0/100] [Batch 553/938] [D loss: 0.058177] [G loss: 4.148464]\n",
      "[Epoch 0/100] [Batch 554/938] [D loss: 0.064582] [G loss: 2.971584]\n",
      "[Epoch 0/100] [Batch 555/938] [D loss: 0.077448] [G loss: 3.030771]\n",
      "[Epoch 0/100] [Batch 556/938] [D loss: 0.087325] [G loss: 3.241901]\n",
      "[Epoch 0/100] [Batch 557/938] [D loss: 0.083595] [G loss: 2.658119]\n",
      "[Epoch 0/100] [Batch 558/938] [D loss: 0.124090] [G loss: 3.769865]\n",
      "[Epoch 0/100] [Batch 559/938] [D loss: 0.348664] [G loss: 0.904894]\n",
      "[Epoch 0/100] [Batch 560/938] [D loss: 2.135887] [G loss: 10.664875]\n",
      "[Epoch 0/100] [Batch 561/938] [D loss: 0.803942] [G loss: 0.364316]\n",
      "[Epoch 0/100] [Batch 562/938] [D loss: 2.618918] [G loss: 8.909378]\n",
      "[Epoch 0/100] [Batch 563/938] [D loss: 0.540963] [G loss: 0.880601]\n",
      "[Epoch 0/100] [Batch 564/938] [D loss: 0.264537] [G loss: 1.409768]\n",
      "[Epoch 0/100] [Batch 565/938] [D loss: 0.262509] [G loss: 2.234052]\n",
      "[Epoch 0/100] [Batch 566/938] [D loss: 0.374255] [G loss: 1.495674]\n",
      "[Epoch 0/100] [Batch 567/938] [D loss: 0.368775] [G loss: 1.161066]\n",
      "[Epoch 0/100] [Batch 568/938] [D loss: 0.241168] [G loss: 1.798213]\n",
      "[Epoch 0/100] [Batch 569/938] [D loss: 0.274536] [G loss: 2.678944]\n",
      "[Epoch 0/100] [Batch 570/938] [D loss: 0.287784] [G loss: 1.173614]\n",
      "[Epoch 0/100] [Batch 571/938] [D loss: 0.183448] [G loss: 2.326166]\n",
      "[Epoch 0/100] [Batch 572/938] [D loss: 0.200376] [G loss: 2.519800]\n",
      "[Epoch 0/100] [Batch 573/938] [D loss: 0.244386] [G loss: 1.314547]\n",
      "[Epoch 0/100] [Batch 574/938] [D loss: 0.268537] [G loss: 2.522653]\n",
      "[Epoch 0/100] [Batch 575/938] [D loss: 0.230393] [G loss: 1.643196]\n",
      "[Epoch 0/100] [Batch 576/938] [D loss: 0.285468] [G loss: 1.893368]\n",
      "[Epoch 0/100] [Batch 577/938] [D loss: 0.178191] [G loss: 2.190480]\n",
      "[Epoch 0/100] [Batch 578/938] [D loss: 0.161609] [G loss: 2.036765]\n",
      "[Epoch 0/100] [Batch 579/938] [D loss: 0.111783] [G loss: 2.462419]\n",
      "[Epoch 0/100] [Batch 580/938] [D loss: 0.143431] [G loss: 2.835149]\n",
      "[Epoch 0/100] [Batch 581/938] [D loss: 0.210601] [G loss: 1.498820]\n",
      "[Epoch 0/100] [Batch 582/938] [D loss: 0.256736] [G loss: 3.343131]\n",
      "[Epoch 0/100] [Batch 583/938] [D loss: 0.442933] [G loss: 0.715704]\n",
      "[Epoch 0/100] [Batch 584/938] [D loss: 0.387917] [G loss: 4.273243]\n",
      "[Epoch 0/100] [Batch 585/938] [D loss: 0.213573] [G loss: 1.413087]\n",
      "[Epoch 0/100] [Batch 586/938] [D loss: 0.130532] [G loss: 2.511040]\n",
      "[Epoch 0/100] [Batch 587/938] [D loss: 0.127647] [G loss: 2.300729]\n",
      "[Epoch 0/100] [Batch 588/938] [D loss: 0.142962] [G loss: 2.749964]\n",
      "[Epoch 0/100] [Batch 589/938] [D loss: 0.194320] [G loss: 1.630812]\n",
      "[Epoch 0/100] [Batch 590/938] [D loss: 0.150899] [G loss: 2.491708]\n",
      "[Epoch 0/100] [Batch 591/938] [D loss: 0.155151] [G loss: 1.962094]\n",
      "[Epoch 0/100] [Batch 592/938] [D loss: 0.155280] [G loss: 2.246550]\n",
      "[Epoch 0/100] [Batch 593/938] [D loss: 0.189925] [G loss: 2.139267]\n",
      "[Epoch 0/100] [Batch 594/938] [D loss: 0.199616] [G loss: 1.729455]\n",
      "[Epoch 0/100] [Batch 595/938] [D loss: 0.237716] [G loss: 2.601675]\n",
      "[Epoch 0/100] [Batch 596/938] [D loss: 0.418732] [G loss: 0.772400]\n",
      "[Epoch 0/100] [Batch 597/938] [D loss: 0.962539] [G loss: 5.715078]\n",
      "[Epoch 0/100] [Batch 598/938] [D loss: 1.073779] [G loss: 0.162816]\n",
      "[Epoch 0/100] [Batch 599/938] [D loss: 0.860930] [G loss: 4.745211]\n",
      "[Epoch 0/100] [Batch 600/938] [D loss: 0.340659] [G loss: 1.188390]\n",
      "[Epoch 0/100] [Batch 601/938] [D loss: 0.351437] [G loss: 1.243565]\n",
      "[Epoch 0/100] [Batch 602/938] [D loss: 0.352254] [G loss: 2.771767]\n",
      "[Epoch 0/100] [Batch 603/938] [D loss: 0.332712] [G loss: 1.167958]\n",
      "[Epoch 0/100] [Batch 604/938] [D loss: 0.238023] [G loss: 1.872344]\n",
      "[Epoch 0/100] [Batch 605/938] [D loss: 0.199758] [G loss: 2.181122]\n",
      "[Epoch 0/100] [Batch 606/938] [D loss: 0.187840] [G loss: 2.063851]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/100] [Batch 607/938] [D loss: 0.255030] [G loss: 1.771655]\n",
      "[Epoch 0/100] [Batch 608/938] [D loss: 0.213484] [G loss: 1.614092]\n",
      "[Epoch 0/100] [Batch 609/938] [D loss: 0.232719] [G loss: 2.880449]\n",
      "[Epoch 0/100] [Batch 610/938] [D loss: 0.193045] [G loss: 1.708395]\n",
      "[Epoch 0/100] [Batch 611/938] [D loss: 0.215492] [G loss: 2.003965]\n",
      "[Epoch 0/100] [Batch 612/938] [D loss: 0.218567] [G loss: 1.894962]\n",
      "[Epoch 0/100] [Batch 613/938] [D loss: 0.230777] [G loss: 1.761261]\n",
      "[Epoch 0/100] [Batch 614/938] [D loss: 0.187196] [G loss: 1.919285]\n",
      "[Epoch 0/100] [Batch 615/938] [D loss: 0.174705] [G loss: 1.923430]\n",
      "[Epoch 0/100] [Batch 616/938] [D loss: 0.193855] [G loss: 2.790509]\n",
      "[Epoch 0/100] [Batch 617/938] [D loss: 0.342289] [G loss: 0.926551]\n",
      "[Epoch 0/100] [Batch 618/938] [D loss: 0.726680] [G loss: 4.571184]\n",
      "[Epoch 0/100] [Batch 619/938] [D loss: 1.039106] [G loss: 0.170739]\n",
      "[Epoch 0/100] [Batch 620/938] [D loss: 1.204351] [G loss: 4.989037]\n",
      "[Epoch 0/100] [Batch 621/938] [D loss: 0.469605] [G loss: 0.695118]\n",
      "[Epoch 0/100] [Batch 622/938] [D loss: 0.305054] [G loss: 2.269166]\n",
      "[Epoch 0/100] [Batch 623/938] [D loss: 0.316465] [G loss: 1.746906]\n",
      "[Epoch 0/100] [Batch 624/938] [D loss: 0.305565] [G loss: 1.151514]\n",
      "[Epoch 0/100] [Batch 625/938] [D loss: 0.393079] [G loss: 1.825694]\n",
      "[Epoch 0/100] [Batch 626/938] [D loss: 0.377265] [G loss: 0.955814]\n",
      "[Epoch 0/100] [Batch 627/938] [D loss: 0.392986] [G loss: 2.647317]\n",
      "[Epoch 0/100] [Batch 628/938] [D loss: 0.430211] [G loss: 0.820034]\n",
      "[Epoch 0/100] [Batch 629/938] [D loss: 0.354646] [G loss: 3.102841]\n",
      "[Epoch 0/100] [Batch 630/938] [D loss: 0.357794] [G loss: 1.023172]\n",
      "[Epoch 0/100] [Batch 631/938] [D loss: 0.212396] [G loss: 2.294468]\n",
      "[Epoch 0/100] [Batch 632/938] [D loss: 0.280187] [G loss: 1.771130]\n",
      "[Epoch 0/100] [Batch 633/938] [D loss: 0.321785] [G loss: 1.130473]\n",
      "[Epoch 0/100] [Batch 634/938] [D loss: 0.379189] [G loss: 3.203739]\n",
      "[Epoch 0/100] [Batch 635/938] [D loss: 0.439417] [G loss: 0.785942]\n",
      "[Epoch 0/100] [Batch 636/938] [D loss: 0.390569] [G loss: 3.034864]\n",
      "[Epoch 0/100] [Batch 637/938] [D loss: 0.439832] [G loss: 0.775920]\n",
      "[Epoch 0/100] [Batch 638/938] [D loss: 0.308565] [G loss: 3.419910]\n",
      "[Epoch 0/100] [Batch 639/938] [D loss: 0.250565] [G loss: 1.310095]\n",
      "[Epoch 0/100] [Batch 640/938] [D loss: 0.212062] [G loss: 1.894098]\n",
      "[Epoch 0/100] [Batch 641/938] [D loss: 0.222093] [G loss: 2.291162]\n",
      "[Epoch 0/100] [Batch 642/938] [D loss: 0.369896] [G loss: 1.051965]\n",
      "[Epoch 0/100] [Batch 643/938] [D loss: 0.452949] [G loss: 3.470645]\n",
      "[Epoch 0/100] [Batch 644/938] [D loss: 0.342404] [G loss: 0.978128]\n",
      "[Epoch 0/100] [Batch 645/938] [D loss: 0.209299] [G loss: 2.429163]\n",
      "[Epoch 0/100] [Batch 646/938] [D loss: 0.229328] [G loss: 2.133249]\n",
      "[Epoch 0/100] [Batch 647/938] [D loss: 0.384872] [G loss: 0.990293]\n",
      "[Epoch 0/100] [Batch 648/938] [D loss: 0.386396] [G loss: 3.552143]\n",
      "[Epoch 0/100] [Batch 649/938] [D loss: 0.413764] [G loss: 0.895709]\n",
      "[Epoch 0/100] [Batch 650/938] [D loss: 0.185440] [G loss: 2.676786]\n",
      "[Epoch 0/100] [Batch 651/938] [D loss: 0.103848] [G loss: 3.025672]\n",
      "[Epoch 0/100] [Batch 652/938] [D loss: 0.293890] [G loss: 1.236474]\n",
      "[Epoch 0/100] [Batch 653/938] [D loss: 0.442800] [G loss: 3.046773]\n",
      "[Epoch 0/100] [Batch 654/938] [D loss: 0.984996] [G loss: 0.196871]\n",
      "[Epoch 0/100] [Batch 655/938] [D loss: 1.036018] [G loss: 5.760820]\n",
      "[Epoch 0/100] [Batch 656/938] [D loss: 0.282914] [G loss: 1.208856]\n",
      "[Epoch 0/100] [Batch 657/938] [D loss: 0.317696] [G loss: 1.043295]\n",
      "[Epoch 0/100] [Batch 658/938] [D loss: 0.414229] [G loss: 2.903543]\n",
      "[Epoch 0/100] [Batch 659/938] [D loss: 0.562677] [G loss: 0.603254]\n",
      "[Epoch 0/100] [Batch 660/938] [D loss: 0.682553] [G loss: 3.147411]\n",
      "[Epoch 0/100] [Batch 661/938] [D loss: 0.604924] [G loss: 0.491500]\n",
      "[Epoch 0/100] [Batch 662/938] [D loss: 0.256749] [G loss: 2.742335]\n",
      "[Epoch 0/100] [Batch 663/938] [D loss: 0.257540] [G loss: 2.471020]\n",
      "[Epoch 0/100] [Batch 664/938] [D loss: 0.275400] [G loss: 1.263609]\n",
      "[Epoch 0/100] [Batch 665/938] [D loss: 0.298767] [G loss: 1.437403]\n",
      "[Epoch 0/100] [Batch 666/938] [D loss: 0.323576] [G loss: 2.003417]\n",
      "[Epoch 0/100] [Batch 667/938] [D loss: 0.362606] [G loss: 1.317191]\n",
      "[Epoch 0/100] [Batch 668/938] [D loss: 0.379654] [G loss: 1.338566]\n",
      "[Epoch 0/100] [Batch 669/938] [D loss: 0.262771] [G loss: 2.107190]\n",
      "[Epoch 0/100] [Batch 670/938] [D loss: 0.243949] [G loss: 1.490663]\n",
      "[Epoch 0/100] [Batch 671/938] [D loss: 0.233771] [G loss: 2.380733]\n",
      "[Epoch 0/100] [Batch 672/938] [D loss: 0.258021] [G loss: 1.630231]\n",
      "[Epoch 0/100] [Batch 673/938] [D loss: 0.170970] [G loss: 1.791340]\n",
      "[Epoch 0/100] [Batch 674/938] [D loss: 0.312935] [G loss: 2.491538]\n",
      "[Epoch 0/100] [Batch 675/938] [D loss: 0.559731] [G loss: 0.486224]\n",
      "[Epoch 0/100] [Batch 676/938] [D loss: 0.684840] [G loss: 4.621409]\n",
      "[Epoch 0/100] [Batch 677/938] [D loss: 0.357128] [G loss: 1.001046]\n",
      "[Epoch 0/100] [Batch 678/938] [D loss: 0.256409] [G loss: 1.738956]\n",
      "[Epoch 0/100] [Batch 679/938] [D loss: 0.313081] [G loss: 2.367277]\n",
      "[Epoch 0/100] [Batch 680/938] [D loss: 0.277488] [G loss: 1.326010]\n",
      "[Epoch 0/100] [Batch 681/938] [D loss: 0.244723] [G loss: 2.111490]\n",
      "[Epoch 0/100] [Batch 682/938] [D loss: 0.200381] [G loss: 2.145307]\n",
      "[Epoch 0/100] [Batch 683/938] [D loss: 0.248794] [G loss: 2.078183]\n",
      "[Epoch 0/100] [Batch 684/938] [D loss: 0.378550] [G loss: 0.777496]\n",
      "[Epoch 0/100] [Batch 685/938] [D loss: 0.493453] [G loss: 4.619709]\n",
      "[Epoch 0/100] [Batch 686/938] [D loss: 0.452435] [G loss: 0.660849]\n",
      "[Epoch 0/100] [Batch 687/938] [D loss: 0.412075] [G loss: 3.135310]\n",
      "[Epoch 0/100] [Batch 688/938] [D loss: 0.354838] [G loss: 0.964628]\n",
      "[Epoch 0/100] [Batch 689/938] [D loss: 0.227577] [G loss: 2.507484]\n",
      "[Epoch 0/100] [Batch 690/938] [D loss: 0.252614] [G loss: 1.867078]\n",
      "[Epoch 0/100] [Batch 691/938] [D loss: 0.282650] [G loss: 1.558439]\n",
      "[Epoch 0/100] [Batch 692/938] [D loss: 0.248539] [G loss: 1.895027]\n",
      "[Epoch 0/100] [Batch 693/938] [D loss: 0.228512] [G loss: 2.020023]\n",
      "[Epoch 0/100] [Batch 694/938] [D loss: 0.206504] [G loss: 1.631922]\n",
      "[Epoch 0/100] [Batch 695/938] [D loss: 0.282058] [G loss: 2.643125]\n",
      "[Epoch 0/100] [Batch 696/938] [D loss: 0.369484] [G loss: 0.834007]\n",
      "[Epoch 0/100] [Batch 697/938] [D loss: 0.422562] [G loss: 4.035368]\n",
      "[Epoch 0/100] [Batch 698/938] [D loss: 0.515521] [G loss: 0.644105]\n",
      "[Epoch 0/100] [Batch 699/938] [D loss: 0.433410] [G loss: 4.318977]\n",
      "[Epoch 0/100] [Batch 700/938] [D loss: 0.293752] [G loss: 1.252481]\n",
      "[Epoch 0/100] [Batch 701/938] [D loss: 0.320808] [G loss: 1.500877]\n",
      "[Epoch 0/100] [Batch 702/938] [D loss: 0.422356] [G loss: 2.064920]\n",
      "[Epoch 0/100] [Batch 703/938] [D loss: 0.429518] [G loss: 0.872636]\n",
      "[Epoch 0/100] [Batch 704/938] [D loss: 0.829933] [G loss: 5.715176]\n",
      "[Epoch 0/100] [Batch 705/938] [D loss: 0.575834] [G loss: 0.502017]\n",
      "[Epoch 0/100] [Batch 706/938] [D loss: 0.324552] [G loss: 3.733211]\n",
      "[Epoch 0/100] [Batch 707/938] [D loss: 0.262956] [G loss: 1.575038]\n",
      "[Epoch 0/100] [Batch 708/938] [D loss: 0.328672] [G loss: 1.326836]\n",
      "[Epoch 0/100] [Batch 709/938] [D loss: 0.545859] [G loss: 1.409428]\n",
      "[Epoch 0/100] [Batch 710/938] [D loss: 0.672105] [G loss: 0.498663]\n",
      "[Epoch 0/100] [Batch 711/938] [D loss: 0.706932] [G loss: 3.155938]\n",
      "[Epoch 0/100] [Batch 712/938] [D loss: 0.622634] [G loss: 0.573608]\n",
      "[Epoch 0/100] [Batch 713/938] [D loss: 0.185665] [G loss: 3.606514]\n",
      "[Epoch 0/100] [Batch 714/938] [D loss: 0.226896] [G loss: 2.634011]\n",
      "[Epoch 0/100] [Batch 715/938] [D loss: 0.373807] [G loss: 0.986605]\n",
      "[Epoch 0/100] [Batch 716/938] [D loss: 0.210241] [G loss: 2.661788]\n",
      "[Epoch 0/100] [Batch 717/938] [D loss: 0.276895] [G loss: 2.072159]\n",
      "[Epoch 0/100] [Batch 718/938] [D loss: 0.354582] [G loss: 1.116524]\n",
      "[Epoch 0/100] [Batch 719/938] [D loss: 0.215360] [G loss: 2.123855]\n",
      "[Epoch 0/100] [Batch 720/938] [D loss: 0.250989] [G loss: 1.681096]\n",
      "[Epoch 0/100] [Batch 721/938] [D loss: 0.234538] [G loss: 1.970571]\n",
      "[Epoch 0/100] [Batch 722/938] [D loss: 0.190326] [G loss: 2.320497]\n",
      "[Epoch 0/100] [Batch 723/938] [D loss: 0.213045] [G loss: 1.758244]\n",
      "[Epoch 0/100] [Batch 724/938] [D loss: 0.205793] [G loss: 2.215223]\n",
      "[Epoch 0/100] [Batch 725/938] [D loss: 0.232847] [G loss: 2.294397]\n",
      "[Epoch 0/100] [Batch 726/938] [D loss: 0.217265] [G loss: 1.401282]\n",
      "[Epoch 0/100] [Batch 727/938] [D loss: 0.343295] [G loss: 2.890222]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/100] [Batch 728/938] [D loss: 0.416485] [G loss: 0.803841]\n",
      "[Epoch 0/100] [Batch 729/938] [D loss: 0.218885] [G loss: 3.422426]\n",
      "[Epoch 0/100] [Batch 730/938] [D loss: 0.316491] [G loss: 1.841898]\n",
      "[Epoch 0/100] [Batch 731/938] [D loss: 0.342813] [G loss: 0.964775]\n",
      "[Epoch 0/100] [Batch 732/938] [D loss: 0.406572] [G loss: 4.714395]\n",
      "[Epoch 0/100] [Batch 733/938] [D loss: 0.192215] [G loss: 1.596091]\n",
      "[Epoch 0/100] [Batch 734/938] [D loss: 0.155273] [G loss: 1.967577]\n",
      "[Epoch 0/100] [Batch 735/938] [D loss: 0.243247] [G loss: 3.465710]\n",
      "[Epoch 0/100] [Batch 736/938] [D loss: 0.245050] [G loss: 1.197407]\n",
      "[Epoch 0/100] [Batch 737/938] [D loss: 0.253686] [G loss: 2.096562]\n",
      "[Epoch 0/100] [Batch 738/938] [D loss: 0.276516] [G loss: 1.795640]\n",
      "[Epoch 0/100] [Batch 739/938] [D loss: 0.232784] [G loss: 1.771985]\n",
      "[Epoch 0/100] [Batch 740/938] [D loss: 0.145139] [G loss: 2.133285]\n",
      "[Epoch 0/100] [Batch 741/938] [D loss: 0.155410] [G loss: 2.766986]\n",
      "[Epoch 0/100] [Batch 742/938] [D loss: 0.149650] [G loss: 2.016931]\n",
      "[Epoch 0/100] [Batch 743/938] [D loss: 0.146540] [G loss: 2.884670]\n",
      "[Epoch 0/100] [Batch 744/938] [D loss: 0.176446] [G loss: 2.376307]\n",
      "[Epoch 0/100] [Batch 745/938] [D loss: 0.125771] [G loss: 2.146042]\n",
      "[Epoch 0/100] [Batch 746/938] [D loss: 0.180784] [G loss: 3.280893]\n",
      "[Epoch 0/100] [Batch 747/938] [D loss: 0.266026] [G loss: 1.196999]\n",
      "[Epoch 0/100] [Batch 748/938] [D loss: 0.348275] [G loss: 4.132400]\n",
      "[Epoch 0/100] [Batch 749/938] [D loss: 0.303022] [G loss: 1.118799]\n",
      "[Epoch 0/100] [Batch 750/938] [D loss: 0.210563] [G loss: 2.528323]\n",
      "[Epoch 0/100] [Batch 751/938] [D loss: 0.195385] [G loss: 2.956933]\n",
      "[Epoch 0/100] [Batch 752/938] [D loss: 0.265290] [G loss: 1.232841]\n",
      "[Epoch 0/100] [Batch 753/938] [D loss: 0.293416] [G loss: 4.233516]\n",
      "[Epoch 0/100] [Batch 754/938] [D loss: 0.313765] [G loss: 1.200707]\n",
      "[Epoch 0/100] [Batch 755/938] [D loss: 0.176344] [G loss: 3.230076]\n",
      "[Epoch 0/100] [Batch 756/938] [D loss: 0.178728] [G loss: 2.397734]\n",
      "[Epoch 0/100] [Batch 757/938] [D loss: 0.257835] [G loss: 1.436742]\n",
      "[Epoch 0/100] [Batch 758/938] [D loss: 0.228292] [G loss: 3.377200]\n",
      "[Epoch 0/100] [Batch 759/938] [D loss: 0.365862] [G loss: 0.914409]\n",
      "[Epoch 0/100] [Batch 760/938] [D loss: 0.508184] [G loss: 4.935186]\n",
      "[Epoch 0/100] [Batch 761/938] [D loss: 0.605467] [G loss: 0.539491]\n",
      "[Epoch 0/100] [Batch 762/938] [D loss: 0.607154] [G loss: 5.221713]\n",
      "[Epoch 0/100] [Batch 763/938] [D loss: 0.454834] [G loss: 0.727582]\n",
      "[Epoch 0/100] [Batch 764/938] [D loss: 0.301158] [G loss: 3.831820]\n",
      "[Epoch 0/100] [Batch 765/938] [D loss: 0.144797] [G loss: 2.265934]\n",
      "[Epoch 0/100] [Batch 766/938] [D loss: 0.195085] [G loss: 1.506783]\n",
      "[Epoch 0/100] [Batch 767/938] [D loss: 0.166677] [G loss: 3.015761]\n",
      "[Epoch 0/100] [Batch 768/938] [D loss: 0.263496] [G loss: 1.649559]\n",
      "[Epoch 0/100] [Batch 769/938] [D loss: 0.174249] [G loss: 2.153365]\n",
      "[Epoch 0/100] [Batch 770/938] [D loss: 0.210742] [G loss: 2.076231]\n",
      "[Epoch 0/100] [Batch 771/938] [D loss: 0.199462] [G loss: 2.306750]\n",
      "[Epoch 0/100] [Batch 772/938] [D loss: 0.202524] [G loss: 1.613396]\n",
      "[Epoch 0/100] [Batch 773/938] [D loss: 0.358471] [G loss: 4.973942]\n",
      "[Epoch 0/100] [Batch 774/938] [D loss: 0.420218] [G loss: 0.754067]\n",
      "[Epoch 0/100] [Batch 775/938] [D loss: 0.150023] [G loss: 4.923248]\n",
      "[Epoch 0/100] [Batch 776/938] [D loss: 0.157223] [G loss: 2.332327]\n",
      "[Epoch 0/100] [Batch 777/938] [D loss: 0.367153] [G loss: 0.979166]\n",
      "[Epoch 0/100] [Batch 778/938] [D loss: 0.343741] [G loss: 3.005794]\n",
      "[Epoch 0/100] [Batch 779/938] [D loss: 0.459681] [G loss: 0.768002]\n",
      "[Epoch 0/100] [Batch 780/938] [D loss: 0.576433] [G loss: 4.522912]\n",
      "[Epoch 0/100] [Batch 781/938] [D loss: 0.347841] [G loss: 0.987748]\n",
      "[Epoch 0/100] [Batch 782/938] [D loss: 0.155078] [G loss: 2.959524]\n",
      "[Epoch 0/100] [Batch 783/938] [D loss: 0.156459] [G loss: 2.883556]\n",
      "[Epoch 0/100] [Batch 784/938] [D loss: 0.206302] [G loss: 1.556866]\n",
      "[Epoch 0/100] [Batch 785/938] [D loss: 0.244085] [G loss: 1.905538]\n",
      "[Epoch 0/100] [Batch 786/938] [D loss: 0.288783] [G loss: 1.694996]\n",
      "[Epoch 0/100] [Batch 787/938] [D loss: 0.232365] [G loss: 2.814731]\n",
      "[Epoch 0/100] [Batch 788/938] [D loss: 0.370001] [G loss: 0.865122]\n",
      "[Epoch 0/100] [Batch 789/938] [D loss: 0.686577] [G loss: 6.682018]\n",
      "[Epoch 0/100] [Batch 790/938] [D loss: 0.490072] [G loss: 0.731419]\n",
      "[Epoch 0/100] [Batch 791/938] [D loss: 0.096643] [G loss: 3.568341]\n",
      "[Epoch 0/100] [Batch 792/938] [D loss: 0.181457] [G loss: 3.340449]\n",
      "[Epoch 0/100] [Batch 793/938] [D loss: 0.592541] [G loss: 0.557158]\n",
      "[Epoch 0/100] [Batch 794/938] [D loss: 0.648287] [G loss: 3.470551]\n",
      "[Epoch 0/100] [Batch 795/938] [D loss: 0.755613] [G loss: 0.419314]\n",
      "[Epoch 0/100] [Batch 796/938] [D loss: 1.097711] [G loss: 5.712107]\n",
      "[Epoch 0/100] [Batch 797/938] [D loss: 0.411279] [G loss: 0.955798]\n",
      "[Epoch 0/100] [Batch 798/938] [D loss: 0.104105] [G loss: 2.572390]\n",
      "[Epoch 0/100] [Batch 799/938] [D loss: 0.184589] [G loss: 2.580740]\n",
      "[Epoch 0/100] [Batch 800/938] [D loss: 0.253805] [G loss: 1.952081]\n",
      "[Epoch 0/100] [Batch 801/938] [D loss: 0.317149] [G loss: 1.070024]\n",
      "[Epoch 0/100] [Batch 802/938] [D loss: 0.527169] [G loss: 3.381720]\n",
      "[Epoch 0/100] [Batch 803/938] [D loss: 0.416079] [G loss: 0.752879]\n",
      "[Epoch 0/100] [Batch 804/938] [D loss: 0.169392] [G loss: 2.119680]\n",
      "[Epoch 0/100] [Batch 805/938] [D loss: 0.299404] [G loss: 3.028083]\n",
      "[Epoch 0/100] [Batch 806/938] [D loss: 0.528348] [G loss: 0.893940]\n",
      "[Epoch 0/100] [Batch 807/938] [D loss: 0.264902] [G loss: 1.446854]\n",
      "[Epoch 0/100] [Batch 808/938] [D loss: 0.433650] [G loss: 2.839528]\n",
      "[Epoch 0/100] [Batch 809/938] [D loss: 0.294319] [G loss: 1.224214]\n",
      "[Epoch 0/100] [Batch 810/938] [D loss: 0.114769] [G loss: 2.974544]\n",
      "[Epoch 0/100] [Batch 811/938] [D loss: 0.166934] [G loss: 2.855532]\n",
      "[Epoch 0/100] [Batch 812/938] [D loss: 0.149022] [G loss: 1.922211]\n",
      "[Epoch 0/100] [Batch 813/938] [D loss: 0.249195] [G loss: 1.486528]\n",
      "[Epoch 0/100] [Batch 814/938] [D loss: 0.300519] [G loss: 2.940761]\n",
      "[Epoch 0/100] [Batch 815/938] [D loss: 0.322576] [G loss: 1.133273]\n",
      "[Epoch 0/100] [Batch 816/938] [D loss: 0.297669] [G loss: 2.754738]\n",
      "[Epoch 0/100] [Batch 817/938] [D loss: 0.272438] [G loss: 1.398057]\n",
      "[Epoch 0/100] [Batch 818/938] [D loss: 0.159522] [G loss: 2.703135]\n",
      "[Epoch 0/100] [Batch 819/938] [D loss: 0.167397] [G loss: 2.773204]\n",
      "[Epoch 0/100] [Batch 820/938] [D loss: 0.128743] [G loss: 2.186525]\n",
      "[Epoch 0/100] [Batch 821/938] [D loss: 0.155771] [G loss: 2.758357]\n",
      "[Epoch 0/100] [Batch 822/938] [D loss: 0.087507] [G loss: 2.848390]\n",
      "[Epoch 0/100] [Batch 823/938] [D loss: 0.108930] [G loss: 2.670007]\n",
      "[Epoch 0/100] [Batch 824/938] [D loss: 0.072878] [G loss: 2.572544]\n",
      "[Epoch 0/100] [Batch 825/938] [D loss: 0.061967] [G loss: 3.566146]\n",
      "[Epoch 0/100] [Batch 826/938] [D loss: 0.052526] [G loss: 3.193332]\n",
      "[Epoch 0/100] [Batch 827/938] [D loss: 0.040712] [G loss: 4.035742]\n",
      "[Epoch 0/100] [Batch 828/938] [D loss: 0.037525] [G loss: 4.050011]\n",
      "[Epoch 0/100] [Batch 829/938] [D loss: 0.089272] [G loss: 2.505232]\n",
      "[Epoch 0/100] [Batch 830/938] [D loss: 0.121726] [G loss: 2.480822]\n",
      "[Epoch 0/100] [Batch 831/938] [D loss: 0.059202] [G loss: 3.507105]\n",
      "[Epoch 0/100] [Batch 832/938] [D loss: 0.053391] [G loss: 5.288198]\n",
      "[Epoch 0/100] [Batch 833/938] [D loss: 0.045057] [G loss: 3.217801]\n",
      "[Epoch 0/100] [Batch 834/938] [D loss: 0.161040] [G loss: 1.653282]\n",
      "[Epoch 0/100] [Batch 835/938] [D loss: 0.265332] [G loss: 5.224091]\n",
      "[Epoch 0/100] [Batch 836/938] [D loss: 0.327045] [G loss: 0.943638]\n",
      "[Epoch 0/100] [Batch 837/938] [D loss: 0.656959] [G loss: 4.961879]\n",
      "[Epoch 0/100] [Batch 838/938] [D loss: 0.764614] [G loss: 0.362903]\n",
      "[Epoch 0/100] [Batch 839/938] [D loss: 1.687663] [G loss: 6.646313]\n",
      "[Epoch 0/100] [Batch 840/938] [D loss: 1.358325] [G loss: 0.117939]\n",
      "[Epoch 0/100] [Batch 841/938] [D loss: 0.409199] [G loss: 3.361654]\n",
      "[Epoch 0/100] [Batch 842/938] [D loss: 0.345991] [G loss: 1.454204]\n",
      "[Epoch 0/100] [Batch 843/938] [D loss: 0.251634] [G loss: 1.580478]\n",
      "[Epoch 0/100] [Batch 844/938] [D loss: 0.238968] [G loss: 2.173216]\n",
      "[Epoch 0/100] [Batch 845/938] [D loss: 0.234077] [G loss: 1.499812]\n",
      "[Epoch 0/100] [Batch 846/938] [D loss: 0.250752] [G loss: 2.848497]\n",
      "[Epoch 0/100] [Batch 847/938] [D loss: 0.397481] [G loss: 0.858416]\n",
      "[Epoch 0/100] [Batch 848/938] [D loss: 0.535123] [G loss: 3.542267]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/100] [Batch 849/938] [D loss: 0.605955] [G loss: 0.537698]\n",
      "[Epoch 0/100] [Batch 850/938] [D loss: 0.298201] [G loss: 2.274061]\n",
      "[Epoch 0/100] [Batch 851/938] [D loss: 0.209747] [G loss: 3.234059]\n",
      "[Epoch 0/100] [Batch 852/938] [D loss: 0.286673] [G loss: 1.276900]\n",
      "[Epoch 0/100] [Batch 853/938] [D loss: 0.216234] [G loss: 2.453211]\n",
      "[Epoch 0/100] [Batch 854/938] [D loss: 0.212411] [G loss: 2.625686]\n",
      "[Epoch 0/100] [Batch 855/938] [D loss: 0.280099] [G loss: 1.318964]\n",
      "[Epoch 0/100] [Batch 856/938] [D loss: 0.309226] [G loss: 2.383149]\n",
      "[Epoch 0/100] [Batch 857/938] [D loss: 0.305480] [G loss: 1.201545]\n",
      "[Epoch 0/100] [Batch 858/938] [D loss: 0.363405] [G loss: 3.891180]\n",
      "[Epoch 0/100] [Batch 859/938] [D loss: 0.201011] [G loss: 1.468822]\n",
      "[Epoch 0/100] [Batch 860/938] [D loss: 0.103033] [G loss: 2.433273]\n",
      "[Epoch 0/100] [Batch 861/938] [D loss: 0.167460] [G loss: 2.568099]\n",
      "[Epoch 0/100] [Batch 862/938] [D loss: 0.280962] [G loss: 1.191191]\n",
      "[Epoch 0/100] [Batch 863/938] [D loss: 0.703609] [G loss: 3.533862]\n",
      "[Epoch 0/100] [Batch 864/938] [D loss: 0.994833] [G loss: 0.227320]\n",
      "[Epoch 0/100] [Batch 865/938] [D loss: 0.706707] [G loss: 7.406595]\n",
      "[Epoch 0/100] [Batch 866/938] [D loss: 0.140236] [G loss: 1.864679]\n",
      "[Epoch 0/100] [Batch 867/938] [D loss: 0.220115] [G loss: 1.422640]\n",
      "[Epoch 0/100] [Batch 868/938] [D loss: 0.338635] [G loss: 1.341933]\n",
      "[Epoch 0/100] [Batch 869/938] [D loss: 0.916184] [G loss: 1.165404]\n",
      "[Epoch 0/100] [Batch 870/938] [D loss: 0.397995] [G loss: 1.089292]\n",
      "[Epoch 0/100] [Batch 871/938] [D loss: 0.321734] [G loss: 2.171718]\n",
      "[Epoch 0/100] [Batch 872/938] [D loss: 0.167846] [G loss: 2.214459]\n",
      "[Epoch 0/100] [Batch 873/938] [D loss: 0.300607] [G loss: 1.322455]\n",
      "[Epoch 0/100] [Batch 874/938] [D loss: 0.193744] [G loss: 3.628381]\n",
      "[Epoch 0/100] [Batch 875/938] [D loss: 0.193713] [G loss: 2.412175]\n",
      "[Epoch 0/100] [Batch 876/938] [D loss: 0.458651] [G loss: 0.693772]\n",
      "[Epoch 0/100] [Batch 877/938] [D loss: 0.573892] [G loss: 5.517675]\n",
      "[Epoch 0/100] [Batch 878/938] [D loss: 0.280206] [G loss: 1.127391]\n",
      "[Epoch 0/100] [Batch 879/938] [D loss: 0.223009] [G loss: 1.440180]\n",
      "[Epoch 0/100] [Batch 880/938] [D loss: 0.286275] [G loss: 2.333617]\n",
      "[Epoch 0/100] [Batch 881/938] [D loss: 0.410531] [G loss: 2.186329]\n",
      "[Epoch 0/100] [Batch 882/938] [D loss: 0.894804] [G loss: 0.285867]\n",
      "[Epoch 0/100] [Batch 883/938] [D loss: 0.958242] [G loss: 4.896874]\n",
      "[Epoch 0/100] [Batch 884/938] [D loss: 0.279650] [G loss: 1.588750]\n",
      "[Epoch 0/100] [Batch 885/938] [D loss: 0.177951] [G loss: 1.846788]\n",
      "[Epoch 0/100] [Batch 886/938] [D loss: 0.201862] [G loss: 2.561980]\n",
      "[Epoch 0/100] [Batch 887/938] [D loss: 0.156814] [G loss: 1.988532]\n",
      "[Epoch 0/100] [Batch 888/938] [D loss: 0.242050] [G loss: 1.561801]\n",
      "[Epoch 0/100] [Batch 889/938] [D loss: 0.161953] [G loss: 2.033491]\n",
      "[Epoch 0/100] [Batch 890/938] [D loss: 0.250161] [G loss: 2.060556]\n",
      "[Epoch 0/100] [Batch 891/938] [D loss: 0.285161] [G loss: 1.649506]\n",
      "[Epoch 0/100] [Batch 892/938] [D loss: 0.358044] [G loss: 1.115565]\n",
      "[Epoch 0/100] [Batch 893/938] [D loss: 0.393042] [G loss: 2.705205]\n",
      "[Epoch 0/100] [Batch 894/938] [D loss: 0.336110] [G loss: 1.026563]\n",
      "[Epoch 0/100] [Batch 895/938] [D loss: 0.221270] [G loss: 2.823509]\n",
      "[Epoch 0/100] [Batch 896/938] [D loss: 0.215503] [G loss: 2.631476]\n",
      "[Epoch 0/100] [Batch 897/938] [D loss: 0.244127] [G loss: 1.265085]\n",
      "[Epoch 0/100] [Batch 898/938] [D loss: 0.209793] [G loss: 2.842932]\n",
      "[Epoch 0/100] [Batch 899/938] [D loss: 0.178756] [G loss: 2.259765]\n",
      "[Epoch 0/100] [Batch 900/938] [D loss: 0.186578] [G loss: 1.911489]\n",
      "[Epoch 0/100] [Batch 901/938] [D loss: 0.179056] [G loss: 2.003583]\n",
      "[Epoch 0/100] [Batch 902/938] [D loss: 0.173346] [G loss: 3.165659]\n",
      "[Epoch 0/100] [Batch 903/938] [D loss: 0.131732] [G loss: 2.156846]\n",
      "[Epoch 0/100] [Batch 904/938] [D loss: 0.114379] [G loss: 2.327331]\n",
      "[Epoch 0/100] [Batch 905/938] [D loss: 0.059278] [G loss: 3.865168]\n",
      "[Epoch 0/100] [Batch 906/938] [D loss: 0.100626] [G loss: 2.910631]\n",
      "[Epoch 0/100] [Batch 907/938] [D loss: 0.055851] [G loss: 3.905097]\n",
      "[Epoch 0/100] [Batch 908/938] [D loss: 0.085251] [G loss: 2.376538]\n",
      "[Epoch 0/100] [Batch 909/938] [D loss: 0.089476] [G loss: 3.565113]\n",
      "[Epoch 0/100] [Batch 910/938] [D loss: 0.036880] [G loss: 4.791049]\n",
      "[Epoch 0/100] [Batch 911/938] [D loss: 0.156650] [G loss: 1.655732]\n",
      "[Epoch 0/100] [Batch 912/938] [D loss: 0.194044] [G loss: 4.313927]\n",
      "[Epoch 0/100] [Batch 913/938] [D loss: 0.022891] [G loss: 5.074653]\n",
      "[Epoch 0/100] [Batch 914/938] [D loss: 0.579504] [G loss: 0.584672]\n",
      "[Epoch 0/100] [Batch 915/938] [D loss: 0.805729] [G loss: 7.781960]\n",
      "[Epoch 0/100] [Batch 916/938] [D loss: 0.047210] [G loss: 3.006509]\n",
      "[Epoch 0/100] [Batch 917/938] [D loss: 0.605236] [G loss: 0.540633]\n",
      "[Epoch 0/100] [Batch 918/938] [D loss: 1.044857] [G loss: 5.390440]\n",
      "[Epoch 0/100] [Batch 919/938] [D loss: 0.629795] [G loss: 0.586063]\n",
      "[Epoch 0/100] [Batch 920/938] [D loss: 0.118040] [G loss: 3.930645]\n",
      "[Epoch 0/100] [Batch 921/938] [D loss: 0.378128] [G loss: 2.311492]\n",
      "[Epoch 0/100] [Batch 922/938] [D loss: 0.336956] [G loss: 1.113724]\n",
      "[Epoch 0/100] [Batch 923/938] [D loss: 0.176348] [G loss: 1.790989]\n",
      "[Epoch 0/100] [Batch 924/938] [D loss: 0.457241] [G loss: 2.810132]\n",
      "[Epoch 0/100] [Batch 925/938] [D loss: 0.354991] [G loss: 0.960028]\n",
      "[Epoch 0/100] [Batch 926/938] [D loss: 0.228289] [G loss: 3.367137]\n",
      "[Epoch 0/100] [Batch 927/938] [D loss: 0.195243] [G loss: 2.065240]\n",
      "[Epoch 0/100] [Batch 928/938] [D loss: 0.216229] [G loss: 1.929590]\n",
      "[Epoch 0/100] [Batch 929/938] [D loss: 0.204631] [G loss: 2.027858]\n",
      "[Epoch 0/100] [Batch 930/938] [D loss: 0.205870] [G loss: 2.025148]\n",
      "[Epoch 0/100] [Batch 931/938] [D loss: 0.177916] [G loss: 1.959682]\n",
      "[Epoch 0/100] [Batch 932/938] [D loss: 0.295749] [G loss: 3.009974]\n",
      "[Epoch 0/100] [Batch 933/938] [D loss: 0.335116] [G loss: 0.868542]\n",
      "[Epoch 0/100] [Batch 934/938] [D loss: 0.269230] [G loss: 3.612732]\n",
      "[Epoch 0/100] [Batch 935/938] [D loss: 0.124577] [G loss: 2.121636]\n",
      "[Epoch 0/100] [Batch 936/938] [D loss: 0.092923] [G loss: 2.594523]\n",
      "[Epoch 0/100] [Batch 937/938] [D loss: 0.111500] [G loss: 5.009012]\n",
      "[Epoch 1/100] [Batch 0/938] [D loss: 0.316698] [G loss: 1.063344]\n",
      "[Epoch 1/100] [Batch 1/938] [D loss: 0.282860] [G loss: 4.768179]\n",
      "[Epoch 1/100] [Batch 2/938] [D loss: 0.039605] [G loss: 3.948650]\n",
      "[Epoch 1/100] [Batch 3/938] [D loss: 0.337969] [G loss: 0.916298]\n",
      "[Epoch 1/100] [Batch 4/938] [D loss: 0.329989] [G loss: 5.530419]\n",
      "[Epoch 1/100] [Batch 5/938] [D loss: 0.048728] [G loss: 3.412629]\n",
      "[Epoch 1/100] [Batch 6/938] [D loss: 0.426211] [G loss: 0.780661]\n",
      "[Epoch 1/100] [Batch 7/938] [D loss: 0.175258] [G loss: 4.509866]\n",
      "[Epoch 1/100] [Batch 8/938] [D loss: 0.242800] [G loss: 6.066960]\n",
      "[Epoch 1/100] [Batch 9/938] [D loss: 0.244233] [G loss: 1.304107]\n",
      "[Epoch 1/100] [Batch 10/938] [D loss: 0.096777] [G loss: 4.747557]\n",
      "[Epoch 1/100] [Batch 11/938] [D loss: 0.065727] [G loss: 4.188999]\n",
      "[Epoch 1/100] [Batch 12/938] [D loss: 0.356155] [G loss: 0.998564]\n",
      "[Epoch 1/100] [Batch 13/938] [D loss: 0.498762] [G loss: 4.686724]\n",
      "[Epoch 1/100] [Batch 14/938] [D loss: 0.527974] [G loss: 0.650283]\n",
      "[Epoch 1/100] [Batch 15/938] [D loss: 0.568968] [G loss: 4.477820]\n",
      "[Epoch 1/100] [Batch 16/938] [D loss: 0.299975] [G loss: 1.079913]\n",
      "[Epoch 1/100] [Batch 17/938] [D loss: 0.192533] [G loss: 2.088295]\n",
      "[Epoch 1/100] [Batch 18/938] [D loss: 0.254938] [G loss: 3.510694]\n",
      "[Epoch 1/100] [Batch 19/938] [D loss: 0.565183] [G loss: 0.703208]\n",
      "[Epoch 1/100] [Batch 20/938] [D loss: 0.556225] [G loss: 4.597468]\n",
      "[Epoch 1/100] [Batch 21/938] [D loss: 0.217966] [G loss: 1.377270]\n",
      "[Epoch 1/100] [Batch 22/938] [D loss: 0.152975] [G loss: 1.810882]\n",
      "[Epoch 1/100] [Batch 23/938] [D loss: 0.366548] [G loss: 4.336443]\n",
      "[Epoch 1/100] [Batch 24/938] [D loss: 0.538804] [G loss: 0.617385]\n",
      "[Epoch 1/100] [Batch 25/938] [D loss: 0.264847] [G loss: 3.698777]\n",
      "[Epoch 1/100] [Batch 26/938] [D loss: 0.157868] [G loss: 2.920565]\n",
      "[Epoch 1/100] [Batch 27/938] [D loss: 0.272559] [G loss: 1.269967]\n",
      "[Epoch 1/100] [Batch 28/938] [D loss: 0.193545] [G loss: 3.462957]\n",
      "[Epoch 1/100] [Batch 29/938] [D loss: 0.072266] [G loss: 4.056726]\n",
      "[Epoch 1/100] [Batch 30/938] [D loss: 0.045321] [G loss: 3.600962]\n",
      "[Epoch 1/100] [Batch 31/938] [D loss: 0.405427] [G loss: 0.858214]\n",
      "[Epoch 1/100] [Batch 32/938] [D loss: 0.791714] [G loss: 6.107557]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/100] [Batch 33/938] [D loss: 0.279038] [G loss: 1.116365]\n",
      "[Epoch 1/100] [Batch 34/938] [D loss: 0.225967] [G loss: 1.730446]\n",
      "[Epoch 1/100] [Batch 35/938] [D loss: 0.393592] [G loss: 3.787144]\n",
      "[Epoch 1/100] [Batch 36/938] [D loss: 0.455405] [G loss: 0.671111]\n",
      "[Epoch 1/100] [Batch 37/938] [D loss: 0.263580] [G loss: 3.662137]\n",
      "[Epoch 1/100] [Batch 38/938] [D loss: 0.126558] [G loss: 2.433519]\n",
      "[Epoch 1/100] [Batch 39/938] [D loss: 0.199548] [G loss: 2.024461]\n",
      "[Epoch 1/100] [Batch 40/938] [D loss: 0.206126] [G loss: 1.624835]\n",
      "[Epoch 1/100] [Batch 41/938] [D loss: 0.184581] [G loss: 3.361121]\n",
      "[Epoch 1/100] [Batch 42/938] [D loss: 0.138105] [G loss: 1.932933]\n",
      "[Epoch 1/100] [Batch 43/938] [D loss: 0.087107] [G loss: 3.721552]\n",
      "[Epoch 1/100] [Batch 44/938] [D loss: 0.061237] [G loss: 4.220090]\n",
      "[Epoch 1/100] [Batch 45/938] [D loss: 0.069023] [G loss: 2.889727]\n",
      "[Epoch 1/100] [Batch 46/938] [D loss: 0.217337] [G loss: 1.344636]\n",
      "[Epoch 1/100] [Batch 47/938] [D loss: 0.300275] [G loss: 4.823700]\n",
      "[Epoch 1/100] [Batch 48/938] [D loss: 0.087566] [G loss: 2.410367]\n",
      "[Epoch 1/100] [Batch 49/938] [D loss: 0.041917] [G loss: 3.630799]\n",
      "[Epoch 1/100] [Batch 50/938] [D loss: 0.022218] [G loss: 5.189785]\n",
      "[Epoch 1/100] [Batch 51/938] [D loss: 0.287830] [G loss: 1.236480]\n",
      "[Epoch 1/100] [Batch 52/938] [D loss: 0.250431] [G loss: 4.370422]\n",
      "[Epoch 1/100] [Batch 53/938] [D loss: 0.027861] [G loss: 4.754324]\n",
      "[Epoch 1/100] [Batch 54/938] [D loss: 0.079158] [G loss: 2.400342]\n",
      "[Epoch 1/100] [Batch 55/938] [D loss: 0.075786] [G loss: 2.381339]\n",
      "[Epoch 1/100] [Batch 56/938] [D loss: 0.113623] [G loss: 4.472553]\n",
      "[Epoch 1/100] [Batch 57/938] [D loss: 0.039037] [G loss: 3.512333]\n",
      "[Epoch 1/100] [Batch 58/938] [D loss: 0.061039] [G loss: 2.707377]\n",
      "[Epoch 1/100] [Batch 59/938] [D loss: 0.022020] [G loss: 4.311111]\n",
      "[Epoch 1/100] [Batch 60/938] [D loss: 0.078368] [G loss: 2.608489]\n",
      "[Epoch 1/100] [Batch 61/938] [D loss: 0.099166] [G loss: 3.264597]\n",
      "[Epoch 1/100] [Batch 62/938] [D loss: 0.030616] [G loss: 5.030169]\n",
      "[Epoch 1/100] [Batch 63/938] [D loss: 0.018317] [G loss: 5.978245]\n",
      "[Epoch 1/100] [Batch 64/938] [D loss: 0.085616] [G loss: 2.250127]\n",
      "[Epoch 1/100] [Batch 65/938] [D loss: 0.071316] [G loss: 3.010288]\n",
      "[Epoch 1/100] [Batch 66/938] [D loss: 0.110448] [G loss: 3.342968]\n",
      "[Epoch 1/100] [Batch 67/938] [D loss: 0.027401] [G loss: 7.602640]\n",
      "[Epoch 1/100] [Batch 68/938] [D loss: 0.096165] [G loss: 2.262409]\n",
      "[Epoch 1/100] [Batch 69/938] [D loss: 0.032856] [G loss: 6.453837]\n",
      "[Epoch 1/100] [Batch 70/938] [D loss: 0.025750] [G loss: 7.582417]\n",
      "[Epoch 1/100] [Batch 71/938] [D loss: 0.064711] [G loss: 3.089150]\n",
      "[Epoch 1/100] [Batch 72/938] [D loss: 0.044404] [G loss: 4.097243]\n",
      "[Epoch 1/100] [Batch 73/938] [D loss: 0.058253] [G loss: 3.299781]\n",
      "[Epoch 1/100] [Batch 74/938] [D loss: 0.055363] [G loss: 3.745247]\n",
      "[Epoch 1/100] [Batch 75/938] [D loss: 0.051250] [G loss: 4.059831]\n",
      "[Epoch 1/100] [Batch 76/938] [D loss: 0.023140] [G loss: 3.893417]\n",
      "[Epoch 1/100] [Batch 77/938] [D loss: 0.015180] [G loss: 4.261860]\n",
      "[Epoch 1/100] [Batch 78/938] [D loss: 0.050945] [G loss: 3.133012]\n",
      "[Epoch 1/100] [Batch 79/938] [D loss: 0.028740] [G loss: 4.025193]\n",
      "[Epoch 1/100] [Batch 80/938] [D loss: 0.030484] [G loss: 4.511584]\n",
      "[Epoch 1/100] [Batch 81/938] [D loss: 0.033488] [G loss: 4.264857]\n",
      "[Epoch 1/100] [Batch 82/938] [D loss: 0.026060] [G loss: 6.973038]\n",
      "[Epoch 1/100] [Batch 83/938] [D loss: 0.014361] [G loss: 4.716818]\n",
      "[Epoch 1/100] [Batch 84/938] [D loss: 0.038253] [G loss: 3.426155]\n",
      "[Epoch 1/100] [Batch 85/938] [D loss: 0.022850] [G loss: 4.234423]\n",
      "[Epoch 1/100] [Batch 86/938] [D loss: 0.022078] [G loss: 7.375776]\n",
      "[Epoch 1/100] [Batch 87/938] [D loss: 0.027757] [G loss: 3.882813]\n",
      "[Epoch 1/100] [Batch 88/938] [D loss: 0.035430] [G loss: 3.339815]\n",
      "[Epoch 1/100] [Batch 89/938] [D loss: 0.025993] [G loss: 5.800923]\n",
      "[Epoch 1/100] [Batch 90/938] [D loss: 0.018960] [G loss: 7.921150]\n",
      "[Epoch 1/100] [Batch 91/938] [D loss: 0.013953] [G loss: 7.144134]\n",
      "[Epoch 1/100] [Batch 92/938] [D loss: 0.071840] [G loss: 2.457079]\n",
      "[Epoch 1/100] [Batch 93/938] [D loss: 0.027738] [G loss: 4.084468]\n",
      "[Epoch 1/100] [Batch 94/938] [D loss: 0.105770] [G loss: 8.874084]\n",
      "[Epoch 1/100] [Batch 95/938] [D loss: 0.185941] [G loss: 1.442468]\n",
      "[Epoch 1/100] [Batch 96/938] [D loss: 0.111230] [G loss: 6.966238]\n",
      "[Epoch 1/100] [Batch 97/938] [D loss: 0.014679] [G loss: 9.278393]\n",
      "[Epoch 1/100] [Batch 98/938] [D loss: 0.060901] [G loss: 2.866827]\n",
      "[Epoch 1/100] [Batch 99/938] [D loss: 0.016246] [G loss: 10.468277]\n",
      "[Epoch 1/100] [Batch 100/938] [D loss: 0.035576] [G loss: 3.622450]\n",
      "[Epoch 1/100] [Batch 101/938] [D loss: 0.021505] [G loss: 10.992908]\n",
      "[Epoch 1/100] [Batch 102/938] [D loss: 0.017907] [G loss: 7.513705]\n",
      "[Epoch 1/100] [Batch 103/938] [D loss: 0.051149] [G loss: 2.958919]\n",
      "[Epoch 1/100] [Batch 104/938] [D loss: 0.025635] [G loss: 4.170677]\n",
      "[Epoch 1/100] [Batch 105/938] [D loss: 0.035919] [G loss: 4.627929]\n",
      "[Epoch 1/100] [Batch 106/938] [D loss: 0.016778] [G loss: 5.604824]\n",
      "[Epoch 1/100] [Batch 107/938] [D loss: 0.046006] [G loss: 3.138355]\n",
      "[Epoch 1/100] [Batch 108/938] [D loss: 0.030338] [G loss: 4.677085]\n",
      "[Epoch 1/100] [Batch 109/938] [D loss: 0.019167] [G loss: 10.809652]\n",
      "[Epoch 1/100] [Batch 110/938] [D loss: 0.017063] [G loss: 4.401412]\n",
      "[Epoch 1/100] [Batch 111/938] [D loss: 0.005047] [G loss: 11.540024]\n",
      "[Epoch 1/100] [Batch 112/938] [D loss: 0.005553] [G loss: 6.983710]\n",
      "[Epoch 1/100] [Batch 113/938] [D loss: 0.003290] [G loss: 7.376712]\n",
      "[Epoch 1/100] [Batch 114/938] [D loss: 0.052285] [G loss: 2.777751]\n",
      "[Epoch 1/100] [Batch 115/938] [D loss: 0.039954] [G loss: 7.974377]\n",
      "[Epoch 1/100] [Batch 116/938] [D loss: 0.022245] [G loss: 4.298176]\n",
      "[Epoch 1/100] [Batch 117/938] [D loss: 0.009012] [G loss: 5.759383]\n",
      "[Epoch 1/100] [Batch 118/938] [D loss: 0.003996] [G loss: 8.479132]\n",
      "[Epoch 1/100] [Batch 119/938] [D loss: 0.105196] [G loss: 1.986931]\n",
      "[Epoch 1/100] [Batch 120/938] [D loss: 0.490118] [G loss: 7.896071]\n",
      "[Epoch 1/100] [Batch 121/938] [D loss: 0.233530] [G loss: 1.325393]\n",
      "[Epoch 1/100] [Batch 122/938] [D loss: 0.172444] [G loss: 3.967414]\n",
      "[Epoch 1/100] [Batch 123/938] [D loss: 1.003346] [G loss: 0.226401]\n",
      "[Epoch 1/100] [Batch 124/938] [D loss: 4.120643] [G loss: 10.666842]\n",
      "[Epoch 1/100] [Batch 125/938] [D loss: 0.383925] [G loss: 2.192114]\n",
      "[Epoch 1/100] [Batch 126/938] [D loss: 0.715825] [G loss: 0.424835]\n",
      "[Epoch 1/100] [Batch 127/938] [D loss: 0.387056] [G loss: 3.666168]\n",
      "[Epoch 1/100] [Batch 128/938] [D loss: 0.304201] [G loss: 1.530656]\n",
      "[Epoch 1/100] [Batch 129/938] [D loss: 0.611166] [G loss: 0.589093]\n",
      "[Epoch 1/100] [Batch 130/938] [D loss: 0.828648] [G loss: 3.512913]\n",
      "[Epoch 1/100] [Batch 131/938] [D loss: 0.599183] [G loss: 0.533140]\n",
      "[Epoch 1/100] [Batch 132/938] [D loss: 0.318838] [G loss: 2.552673]\n",
      "[Epoch 1/100] [Batch 133/938] [D loss: 0.245650] [G loss: 1.541246]\n",
      "[Epoch 1/100] [Batch 134/938] [D loss: 0.246298] [G loss: 2.809097]\n",
      "[Epoch 1/100] [Batch 135/938] [D loss: 0.292060] [G loss: 1.153336]\n",
      "[Epoch 1/100] [Batch 136/938] [D loss: 0.243391] [G loss: 2.648931]\n",
      "[Epoch 1/100] [Batch 137/938] [D loss: 0.198949] [G loss: 1.751970]\n",
      "[Epoch 1/100] [Batch 138/938] [D loss: 0.212643] [G loss: 2.213758]\n",
      "[Epoch 1/100] [Batch 139/938] [D loss: 0.286039] [G loss: 1.349061]\n",
      "[Epoch 1/100] [Batch 140/938] [D loss: 0.523890] [G loss: 3.582996]\n",
      "[Epoch 1/100] [Batch 141/938] [D loss: 0.967333] [G loss: 0.245830]\n",
      "[Epoch 1/100] [Batch 142/938] [D loss: 0.577475] [G loss: 6.008418]\n",
      "[Epoch 1/100] [Batch 143/938] [D loss: 0.098868] [G loss: 2.748688]\n",
      "[Epoch 1/100] [Batch 144/938] [D loss: 0.274602] [G loss: 1.204316]\n",
      "[Epoch 1/100] [Batch 145/938] [D loss: 0.194065] [G loss: 2.251578]\n",
      "[Epoch 1/100] [Batch 146/938] [D loss: 0.151843] [G loss: 2.925643]\n",
      "[Epoch 1/100] [Batch 147/938] [D loss: 0.326139] [G loss: 1.420845]\n",
      "[Epoch 1/100] [Batch 148/938] [D loss: 0.370577] [G loss: 2.296439]\n",
      "[Epoch 1/100] [Batch 149/938] [D loss: 0.652274] [G loss: 0.461010]\n",
      "[Epoch 1/100] [Batch 150/938] [D loss: 0.701509] [G loss: 4.612831]\n",
      "[Epoch 1/100] [Batch 151/938] [D loss: 0.333123] [G loss: 1.013515]\n",
      "[Epoch 1/100] [Batch 152/938] [D loss: 0.157757] [G loss: 2.930250]\n",
      "[Epoch 1/100] [Batch 153/938] [D loss: 0.174272] [G loss: 2.621358]\n",
      "[Epoch 1/100] [Batch 154/938] [D loss: 0.202464] [G loss: 1.548903]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/100] [Batch 155/938] [D loss: 0.142891] [G loss: 2.725489]\n",
      "[Epoch 1/100] [Batch 156/938] [D loss: 0.324596] [G loss: 2.242314]\n",
      "[Epoch 1/100] [Batch 157/938] [D loss: 0.527389] [G loss: 0.579139]\n",
      "[Epoch 1/100] [Batch 158/938] [D loss: 0.548541] [G loss: 4.108606]\n",
      "[Epoch 1/100] [Batch 159/938] [D loss: 0.406155] [G loss: 0.989192]\n",
      "[Epoch 1/100] [Batch 160/938] [D loss: 0.185158] [G loss: 2.760341]\n",
      "[Epoch 1/100] [Batch 161/938] [D loss: 0.270115] [G loss: 1.831155]\n",
      "[Epoch 1/100] [Batch 162/938] [D loss: 0.239521] [G loss: 1.853852]\n",
      "[Epoch 1/100] [Batch 163/938] [D loss: 0.196648] [G loss: 2.065266]\n",
      "[Epoch 1/100] [Batch 164/938] [D loss: 0.137650] [G loss: 2.461432]\n",
      "[Epoch 1/100] [Batch 165/938] [D loss: 0.121354] [G loss: 3.214366]\n",
      "[Epoch 1/100] [Batch 166/938] [D loss: 0.267977] [G loss: 1.390039]\n",
      "[Epoch 1/100] [Batch 167/938] [D loss: 0.515161] [G loss: 4.217528]\n",
      "[Epoch 1/100] [Batch 168/938] [D loss: 1.257160] [G loss: 0.124433]\n",
      "[Epoch 1/100] [Batch 169/938] [D loss: 0.749105] [G loss: 5.027124]\n",
      "[Epoch 1/100] [Batch 170/938] [D loss: 0.257360] [G loss: 1.790678]\n",
      "[Epoch 1/100] [Batch 171/938] [D loss: 0.300534] [G loss: 1.180107]\n",
      "[Epoch 1/100] [Batch 172/938] [D loss: 0.239732] [G loss: 3.131687]\n",
      "[Epoch 1/100] [Batch 173/938] [D loss: 0.236010] [G loss: 1.940979]\n",
      "[Epoch 1/100] [Batch 174/938] [D loss: 0.246703] [G loss: 1.624146]\n",
      "[Epoch 1/100] [Batch 175/938] [D loss: 0.242722] [G loss: 1.584140]\n",
      "[Epoch 1/100] [Batch 176/938] [D loss: 0.213629] [G loss: 2.364775]\n",
      "[Epoch 1/100] [Batch 177/938] [D loss: 0.153143] [G loss: 2.411209]\n",
      "[Epoch 1/100] [Batch 178/938] [D loss: 0.285078] [G loss: 1.325962]\n",
      "[Epoch 1/100] [Batch 179/938] [D loss: 0.332588] [G loss: 3.536204]\n",
      "[Epoch 1/100] [Batch 180/938] [D loss: 0.218004] [G loss: 1.446045]\n",
      "[Epoch 1/100] [Batch 181/938] [D loss: 0.100657] [G loss: 3.179039]\n",
      "[Epoch 1/100] [Batch 182/938] [D loss: 0.145372] [G loss: 3.088156]\n",
      "[Epoch 1/100] [Batch 183/938] [D loss: 0.208971] [G loss: 1.573440]\n",
      "[Epoch 1/100] [Batch 184/938] [D loss: 0.176017] [G loss: 2.673440]\n",
      "[Epoch 1/100] [Batch 185/938] [D loss: 0.100395] [G loss: 2.671039]\n",
      "[Epoch 1/100] [Batch 186/938] [D loss: 0.282948] [G loss: 1.603084]\n",
      "[Epoch 1/100] [Batch 187/938] [D loss: 0.247039] [G loss: 3.173703]\n",
      "[Epoch 1/100] [Batch 188/938] [D loss: 0.427469] [G loss: 0.805351]\n",
      "[Epoch 1/100] [Batch 189/938] [D loss: 0.546528] [G loss: 5.492373]\n",
      "[Epoch 1/100] [Batch 190/938] [D loss: 0.315723] [G loss: 1.036008]\n",
      "[Epoch 1/100] [Batch 191/938] [D loss: 0.134023] [G loss: 3.090359]\n",
      "[Epoch 1/100] [Batch 192/938] [D loss: 0.151724] [G loss: 3.357749]\n",
      "[Epoch 1/100] [Batch 193/938] [D loss: 0.332432] [G loss: 1.012018]\n",
      "[Epoch 1/100] [Batch 194/938] [D loss: 0.457691] [G loss: 3.075974]\n",
      "[Epoch 1/100] [Batch 195/938] [D loss: 0.379277] [G loss: 0.865131]\n",
      "[Epoch 1/100] [Batch 196/938] [D loss: 0.223538] [G loss: 4.280830]\n",
      "[Epoch 1/100] [Batch 197/938] [D loss: 0.168626] [G loss: 1.956417]\n",
      "[Epoch 1/100] [Batch 198/938] [D loss: 0.179146] [G loss: 1.936948]\n",
      "[Epoch 1/100] [Batch 199/938] [D loss: 0.172780] [G loss: 2.603040]\n",
      "[Epoch 1/100] [Batch 200/938] [D loss: 0.269620] [G loss: 1.836622]\n",
      "[Epoch 1/100] [Batch 201/938] [D loss: 0.207797] [G loss: 1.718820]\n",
      "[Epoch 1/100] [Batch 202/938] [D loss: 0.416162] [G loss: 3.177430]\n",
      "[Epoch 1/100] [Batch 203/938] [D loss: 0.521330] [G loss: 0.565865]\n",
      "[Epoch 1/100] [Batch 204/938] [D loss: 0.198531] [G loss: 4.940628]\n",
      "[Epoch 1/100] [Batch 205/938] [D loss: 0.191035] [G loss: 5.035289]\n",
      "[Epoch 1/100] [Batch 206/938] [D loss: 0.180272] [G loss: 1.490398]\n",
      "[Epoch 1/100] [Batch 207/938] [D loss: 0.237218] [G loss: 1.370773]\n",
      "[Epoch 1/100] [Batch 208/938] [D loss: 0.516834] [G loss: 4.662431]\n",
      "[Epoch 1/100] [Batch 209/938] [D loss: 0.707373] [G loss: 0.457203]\n",
      "[Epoch 1/100] [Batch 210/938] [D loss: 0.488805] [G loss: 6.129258]\n",
      "[Epoch 1/100] [Batch 211/938] [D loss: 0.119824] [G loss: 2.171754]\n",
      "[Epoch 1/100] [Batch 212/938] [D loss: 0.321049] [G loss: 1.058759]\n",
      "[Epoch 1/100] [Batch 213/938] [D loss: 0.489237] [G loss: 4.098132]\n",
      "[Epoch 1/100] [Batch 214/938] [D loss: 0.345453] [G loss: 1.015419]\n",
      "[Epoch 1/100] [Batch 215/938] [D loss: 0.382333] [G loss: 2.397557]\n",
      "[Epoch 1/100] [Batch 216/938] [D loss: 0.385010] [G loss: 1.009648]\n",
      "[Epoch 1/100] [Batch 217/938] [D loss: 0.353276] [G loss: 4.201260]\n",
      "[Epoch 1/100] [Batch 218/938] [D loss: 0.248983] [G loss: 1.528031]\n",
      "[Epoch 1/100] [Batch 219/938] [D loss: 0.120681] [G loss: 2.610642]\n",
      "[Epoch 1/100] [Batch 220/938] [D loss: 0.087342] [G loss: 3.458329]\n",
      "[Epoch 1/100] [Batch 221/938] [D loss: 0.102885] [G loss: 2.689303]\n",
      "[Epoch 1/100] [Batch 222/938] [D loss: 0.158445] [G loss: 1.940908]\n",
      "[Epoch 1/100] [Batch 223/938] [D loss: 0.306368] [G loss: 3.497668]\n",
      "[Epoch 1/100] [Batch 224/938] [D loss: 0.801698] [G loss: 0.363172]\n",
      "[Epoch 1/100] [Batch 225/938] [D loss: 0.862954] [G loss: 5.278522]\n",
      "[Epoch 1/100] [Batch 226/938] [D loss: 0.388852] [G loss: 0.942531]\n",
      "[Epoch 1/100] [Batch 227/938] [D loss: 0.156106] [G loss: 2.623209]\n",
      "[Epoch 1/100] [Batch 228/938] [D loss: 0.221275] [G loss: 3.010601]\n",
      "[Epoch 1/100] [Batch 229/938] [D loss: 0.224057] [G loss: 1.508677]\n",
      "[Epoch 1/100] [Batch 230/938] [D loss: 0.146631] [G loss: 2.475772]\n",
      "[Epoch 1/100] [Batch 231/938] [D loss: 0.208911] [G loss: 3.160010]\n",
      "[Epoch 1/100] [Batch 232/938] [D loss: 0.211767] [G loss: 1.532433]\n",
      "[Epoch 1/100] [Batch 233/938] [D loss: 0.154048] [G loss: 3.539595]\n",
      "[Epoch 1/100] [Batch 234/938] [D loss: 0.089676] [G loss: 2.704377]\n",
      "[Epoch 1/100] [Batch 235/938] [D loss: 0.273702] [G loss: 1.197279]\n",
      "[Epoch 1/100] [Batch 236/938] [D loss: 0.458190] [G loss: 3.628581]\n",
      "[Epoch 1/100] [Batch 237/938] [D loss: 0.421394] [G loss: 0.775047]\n",
      "[Epoch 1/100] [Batch 238/938] [D loss: 0.187473] [G loss: 3.019876]\n",
      "[Epoch 1/100] [Batch 239/938] [D loss: 0.169197] [G loss: 3.352620]\n",
      "[Epoch 1/100] [Batch 240/938] [D loss: 0.204017] [G loss: 1.606021]\n",
      "[Epoch 1/100] [Batch 241/938] [D loss: 0.084358] [G loss: 4.235712]\n",
      "[Epoch 1/100] [Batch 242/938] [D loss: 0.143205] [G loss: 3.877119]\n",
      "[Epoch 1/100] [Batch 243/938] [D loss: 0.228908] [G loss: 1.378970]\n",
      "[Epoch 1/100] [Batch 244/938] [D loss: 0.094189] [G loss: 3.106081]\n",
      "[Epoch 1/100] [Batch 245/938] [D loss: 0.215505] [G loss: 2.947104]\n",
      "[Epoch 1/100] [Batch 246/938] [D loss: 0.341194] [G loss: 1.023118]\n",
      "[Epoch 1/100] [Batch 247/938] [D loss: 0.477295] [G loss: 3.497922]\n",
      "[Epoch 1/100] [Batch 248/938] [D loss: 0.610449] [G loss: 0.500456]\n",
      "[Epoch 1/100] [Batch 249/938] [D loss: 0.504721] [G loss: 5.810704]\n",
      "[Epoch 1/100] [Batch 250/938] [D loss: 0.181368] [G loss: 1.854404]\n",
      "[Epoch 1/100] [Batch 251/938] [D loss: 0.112584] [G loss: 2.416256]\n",
      "[Epoch 1/100] [Batch 252/938] [D loss: 0.122556] [G loss: 2.883604]\n",
      "[Epoch 1/100] [Batch 253/938] [D loss: 0.261755] [G loss: 1.414248]\n",
      "[Epoch 1/100] [Batch 254/938] [D loss: 0.241426] [G loss: 3.323370]\n",
      "[Epoch 1/100] [Batch 255/938] [D loss: 0.268512] [G loss: 1.252560]\n",
      "[Epoch 1/100] [Batch 256/938] [D loss: 0.170633] [G loss: 3.389287]\n",
      "[Epoch 1/100] [Batch 257/938] [D loss: 0.123073] [G loss: 2.277819]\n",
      "[Epoch 1/100] [Batch 258/938] [D loss: 0.122782] [G loss: 2.355913]\n",
      "[Epoch 1/100] [Batch 259/938] [D loss: 0.226138] [G loss: 1.737798]\n",
      "[Epoch 1/100] [Batch 260/938] [D loss: 0.218504] [G loss: 2.815753]\n",
      "[Epoch 1/100] [Batch 261/938] [D loss: 0.165810] [G loss: 1.831567]\n",
      "[Epoch 1/100] [Batch 262/938] [D loss: 0.083658] [G loss: 3.935707]\n",
      "[Epoch 1/100] [Batch 263/938] [D loss: 0.095519] [G loss: 2.886809]\n",
      "[Epoch 1/100] [Batch 264/938] [D loss: 0.130960] [G loss: 2.085022]\n",
      "[Epoch 1/100] [Batch 265/938] [D loss: 0.185348] [G loss: 2.426323]\n",
      "[Epoch 1/100] [Batch 266/938] [D loss: 0.179201] [G loss: 1.834315]\n",
      "[Epoch 1/100] [Batch 267/938] [D loss: 0.304248] [G loss: 3.595735]\n",
      "[Epoch 1/100] [Batch 268/938] [D loss: 0.735352] [G loss: 0.402527]\n",
      "[Epoch 1/100] [Batch 269/938] [D loss: 0.921478] [G loss: 8.761820]\n",
      "[Epoch 1/100] [Batch 270/938] [D loss: 0.263793] [G loss: 1.247618]\n",
      "[Epoch 1/100] [Batch 271/938] [D loss: 0.095820] [G loss: 2.676383]\n",
      "[Epoch 1/100] [Batch 272/938] [D loss: 0.256482] [G loss: 3.041100]\n",
      "[Epoch 1/100] [Batch 273/938] [D loss: 0.543227] [G loss: 0.653206]\n",
      "[Epoch 1/100] [Batch 274/938] [D loss: 0.499549] [G loss: 4.419261]\n",
      "[Epoch 1/100] [Batch 275/938] [D loss: 0.546233] [G loss: 0.584949]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/100] [Batch 276/938] [D loss: 0.300715] [G loss: 4.167032]\n",
      "[Epoch 1/100] [Batch 277/938] [D loss: 0.175407] [G loss: 2.366552]\n",
      "[Epoch 1/100] [Batch 278/938] [D loss: 0.209454] [G loss: 1.543098]\n",
      "[Epoch 1/100] [Batch 279/938] [D loss: 0.269035] [G loss: 3.112659]\n",
      "[Epoch 1/100] [Batch 280/938] [D loss: 0.437856] [G loss: 0.773426]\n",
      "[Epoch 1/100] [Batch 281/938] [D loss: 0.365121] [G loss: 4.338806]\n",
      "[Epoch 1/100] [Batch 282/938] [D loss: 0.205885] [G loss: 1.544822]\n",
      "[Epoch 1/100] [Batch 283/938] [D loss: 0.176969] [G loss: 2.496137]\n",
      "[Epoch 1/100] [Batch 284/938] [D loss: 0.151153] [G loss: 2.257500]\n",
      "[Epoch 1/100] [Batch 285/938] [D loss: 0.169732] [G loss: 2.009007]\n",
      "[Epoch 1/100] [Batch 286/938] [D loss: 0.090921] [G loss: 4.113435]\n",
      "[Epoch 1/100] [Batch 287/938] [D loss: 0.206706] [G loss: 2.226676]\n",
      "[Epoch 1/100] [Batch 288/938] [D loss: 0.202887] [G loss: 1.527756]\n",
      "[Epoch 1/100] [Batch 289/938] [D loss: 0.162724] [G loss: 3.417068]\n",
      "[Epoch 1/100] [Batch 290/938] [D loss: 0.190451] [G loss: 1.693538]\n",
      "[Epoch 1/100] [Batch 291/938] [D loss: 0.181819] [G loss: 3.118952]\n",
      "[Epoch 1/100] [Batch 292/938] [D loss: 0.154989] [G loss: 1.856901]\n",
      "[Epoch 1/100] [Batch 293/938] [D loss: 0.121352] [G loss: 3.915788]\n",
      "[Epoch 1/100] [Batch 294/938] [D loss: 0.120155] [G loss: 2.541809]\n",
      "[Epoch 1/100] [Batch 295/938] [D loss: 0.119837] [G loss: 2.395197]\n",
      "[Epoch 1/100] [Batch 296/938] [D loss: 0.092405] [G loss: 2.917684]\n",
      "[Epoch 1/100] [Batch 297/938] [D loss: 0.154843] [G loss: 2.738774]\n",
      "[Epoch 1/100] [Batch 298/938] [D loss: 0.102463] [G loss: 2.246425]\n",
      "[Epoch 1/100] [Batch 299/938] [D loss: 0.088359] [G loss: 3.716245]\n",
      "[Epoch 1/100] [Batch 300/938] [D loss: 0.055795] [G loss: 3.737016]\n",
      "[Epoch 1/100] [Batch 301/938] [D loss: 0.038838] [G loss: 4.393687]\n",
      "[Epoch 1/100] [Batch 302/938] [D loss: 0.159620] [G loss: 1.568425]\n",
      "[Epoch 1/100] [Batch 303/938] [D loss: 0.126356] [G loss: 4.664529]\n",
      "[Epoch 1/100] [Batch 304/938] [D loss: 0.051336] [G loss: 4.261969]\n",
      "[Epoch 1/100] [Batch 305/938] [D loss: 0.085032] [G loss: 2.442479]\n",
      "[Epoch 1/100] [Batch 306/938] [D loss: 0.040925] [G loss: 3.436331]\n",
      "[Epoch 1/100] [Batch 307/938] [D loss: 0.042617] [G loss: 4.320004]\n",
      "[Epoch 1/100] [Batch 308/938] [D loss: 0.029539] [G loss: 4.744203]\n",
      "[Epoch 1/100] [Batch 309/938] [D loss: 0.053867] [G loss: 2.902475]\n",
      "[Epoch 1/100] [Batch 310/938] [D loss: 0.046095] [G loss: 4.266686]\n",
      "[Epoch 1/100] [Batch 311/938] [D loss: 0.101775] [G loss: 2.244413]\n",
      "[Epoch 1/100] [Batch 312/938] [D loss: 0.208752] [G loss: 4.956217]\n",
      "[Epoch 1/100] [Batch 313/938] [D loss: 0.404553] [G loss: 0.730824]\n",
      "[Epoch 1/100] [Batch 314/938] [D loss: 0.960510] [G loss: 8.225704]\n",
      "[Epoch 1/100] [Batch 315/938] [D loss: 0.302300] [G loss: 1.127989]\n",
      "[Epoch 1/100] [Batch 316/938] [D loss: 0.099895] [G loss: 3.621952]\n",
      "[Epoch 1/100] [Batch 317/938] [D loss: 0.193728] [G loss: 2.327476]\n",
      "[Epoch 1/100] [Batch 318/938] [D loss: 0.451492] [G loss: 0.783316]\n",
      "[Epoch 1/100] [Batch 319/938] [D loss: 1.696578] [G loss: 5.401800]\n",
      "[Epoch 1/100] [Batch 320/938] [D loss: 2.442095] [G loss: 0.018023]\n",
      "[Epoch 1/100] [Batch 321/938] [D loss: 0.199330] [G loss: 3.876160]\n",
      "[Epoch 1/100] [Batch 322/938] [D loss: 0.233291] [G loss: 3.144698]\n",
      "[Epoch 1/100] [Batch 323/938] [D loss: 0.260381] [G loss: 1.665678]\n",
      "[Epoch 1/100] [Batch 324/938] [D loss: 0.539712] [G loss: 0.601462]\n",
      "[Epoch 1/100] [Batch 325/938] [D loss: 0.573241] [G loss: 3.197334]\n",
      "[Epoch 1/100] [Batch 326/938] [D loss: 0.338237] [G loss: 1.074858]\n",
      "[Epoch 1/100] [Batch 327/938] [D loss: 0.439061] [G loss: 0.832123]\n",
      "[Epoch 1/100] [Batch 328/938] [D loss: 0.666497] [G loss: 2.302703]\n",
      "[Epoch 1/100] [Batch 329/938] [D loss: 0.483943] [G loss: 0.798481]\n",
      "[Epoch 1/100] [Batch 330/938] [D loss: 0.418271] [G loss: 1.814542]\n",
      "[Epoch 1/100] [Batch 331/938] [D loss: 0.302511] [G loss: 1.577116]\n",
      "[Epoch 1/100] [Batch 332/938] [D loss: 0.242929] [G loss: 1.827078]\n",
      "[Epoch 1/100] [Batch 333/938] [D loss: 0.229898] [G loss: 1.845076]\n",
      "[Epoch 1/100] [Batch 334/938] [D loss: 0.148536] [G loss: 2.762330]\n",
      "[Epoch 1/100] [Batch 335/938] [D loss: 0.232565] [G loss: 1.708561]\n",
      "[Epoch 1/100] [Batch 336/938] [D loss: 0.177021] [G loss: 2.498034]\n",
      "[Epoch 1/100] [Batch 337/938] [D loss: 0.127171] [G loss: 2.219158]\n",
      "[Epoch 1/100] [Batch 338/938] [D loss: 0.256054] [G loss: 2.139271]\n",
      "[Epoch 1/100] [Batch 339/938] [D loss: 0.218807] [G loss: 1.502723]\n",
      "[Epoch 1/100] [Batch 340/938] [D loss: 0.200194] [G loss: 2.471428]\n",
      "[Epoch 1/100] [Batch 341/938] [D loss: 0.305389] [G loss: 2.177580]\n",
      "[Epoch 1/100] [Batch 342/938] [D loss: 0.503465] [G loss: 0.702354]\n",
      "[Epoch 1/100] [Batch 343/938] [D loss: 0.780610] [G loss: 4.657933]\n",
      "[Epoch 1/100] [Batch 344/938] [D loss: 0.366569] [G loss: 1.013661]\n",
      "[Epoch 1/100] [Batch 345/938] [D loss: 0.196441] [G loss: 1.799062]\n",
      "[Epoch 1/100] [Batch 346/938] [D loss: 0.257866] [G loss: 4.209134]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-155-8be1730358aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Loss measures generator's ability to fool the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# discriminator1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madversarial_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mg_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.0/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.0/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    502\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.0/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2026\u001b[0m     return torch._C._nn.binary_cross_entropy(\n\u001b[0;32m-> 2027\u001b[0;31m         input, target, weight, reduction_enum)\n\u001b[0m\u001b[1;32m   2028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "opt.n_epochs = 100\n",
    "writer = SummaryWriter()\n",
    "for epoch in range(opt.n_epochs):\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "        # Adversarial ground truths\n",
    "        # 10batch_size\n",
    "        valid = Variable(Tensor(imgs.size(0), 1, 1, 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(Tensor(imgs.size(0), 1, 1, 1).fill_(0.0), requires_grad=False)\n",
    "        \n",
    "        # Configure input \n",
    "        # real_imgstypetensor\n",
    "        real_imgs = Variable(imgs.type(Tensor))\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], opt.latent_dim, 1, 1))))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z)\n",
    "        \n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        # discriminator1\n",
    "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
    "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\" % (epoch, opt.n_epochs, i, len(dataloader),\n",
    "                                                            d_loss.item(), g_loss.item()))\n",
    "\n",
    "        x = make_grid(gen_imgs.data[:25], normalize=True, scale_each=True)\n",
    "        writer.add_image('Image', x, i)\n",
    "        writer.add_scalar(\"g_loss\", g_loss.item(), i*epoch)\n",
    "        writer.add_scalar(\"d_loss\", d_loss.item(), i*epoch)\n",
    "        \n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "#         if batches_done % opt.sample_interval == 0:\n",
    "#             save_image(gen_imgs.data[:25], 'images/%d.png' % batches_done, nrow=5, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
